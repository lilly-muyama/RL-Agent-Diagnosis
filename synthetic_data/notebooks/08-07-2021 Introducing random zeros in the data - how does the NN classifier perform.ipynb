{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fc93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7828f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c309f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    '''Calculate roc_auc score'''\n",
    "    fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "    target= list(class_dict.keys())\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "\n",
    "    for (idx, c_label) in enumerate(target):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    plt.close()\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9a63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {'A': 0, 'B': 0, 'C': 0}\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['A'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['B'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['C'] += 1           \n",
    "        else:\n",
    "            print('Check classes')\n",
    "            \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca994bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6d77e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71428571, 0.07692308, 0.83673469],\n",
       "       [0.85714286, 0.23076923, 0.33673469],\n",
       "       [0.39285714, 0.69230769, 0.69387755],\n",
       "       ...,\n",
       "       [0.        , 0.30769231, 0.76530612],\n",
       "       [0.5       , 0.30769231, 0.2244898 ],\n",
       "       [0.        , 0.38461538, 0.44897959]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_df = pd.read_csv('data/dataset_10000.csv')\n",
    "\n",
    "class_dict = {'A':0, 'B':1, 'C':2}\n",
    "nn_df['label'].replace(class_dict, inplace=True)\n",
    "\n",
    "X = nn_df.iloc[:, 0:-1]\n",
    "y = nn_df.iloc[:, -1]\n",
    "\n",
    "#X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "#X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "#X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7efd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly replacing some values in dataset with 0s\n",
    "def insert_zeros(arr):\n",
    "    df = pd.DataFrame(arr)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].sample(frac=0.5)\n",
    "    df.fillna(0, inplace=True)\n",
    "    #df.loc[ df.sample(frac=.5).index, 'value'] = 0\n",
    "    #new_arr = arr.flat[np.random.choice([0], len(arr)//2 , replace=False)] = 0\n",
    "    return np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b26bcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.85714286, 0.        , 0.        ],\n",
       "       [0.        , 0.69230769, 0.69387755],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.76530612],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.38461538, 0.44897959]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = insert_zeros(X_train)\n",
    "X_val = insert_zeros(X_val)\n",
    "X_test = insert_zeros(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea50778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving to file\n",
    "np.savetxt('data/zeros/X_train.txt', X_train, fmt='%d')\n",
    "np.savetxt('data/zeros/X_val.txt', X_val, fmt='%d')\n",
    "np.savetxt('data/zeros/X_test.txt', X_test, fmt='%d')\n",
    "\n",
    "np.savetxt('data/zeros/y_train.txt', y_train, fmt='%d')\n",
    "np.savetxt('data/zeros/y_val.txt', y_val, fmt='%d')\n",
    "np.savetxt('data/zeros/y_test.txt', y_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c193a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(np.array(y_train)).long())\n",
    "\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(np.array(y_test)).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33cede57",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "    \n",
    "target_list = torch.tensor(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9857958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0004, 0.0003, 0.0008])\n"
     ]
    }
   ],
   "source": [
    "class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "print(class_weights)\n",
    "###################### OUTPUT ######################tensor([0.1429, 0.0263, 0.0020, 0.0022, 0.0070, 0.0714])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41a07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_all = class_weights[target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77367e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a9f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0007\n",
    "NUM_FEATURES = len(X.columns)\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3a311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler)\n",
    "\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502ace99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, num_class) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0fb338d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd079994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MulticlassClassification(\n",
       "  (layer_1): Linear(in_features=3, out_features=512, bias=True)\n",
       "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer_out): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da3756bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ff3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "740d45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e266c802f6f45238dc56540434d124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 0.85879 | Val Loss: 1.04060 | Train Acc: 49.700| Val Acc: 42.125\n",
      "Epoch 002: | Train Loss: 0.80408 | Val Loss: 1.02774 | Train Acc: 52.553| Val Acc: 44.250\n",
      "Epoch 003: | Train Loss: 0.78021 | Val Loss: 0.99059 | Train Acc: 53.558| Val Acc: 46.500\n",
      "Epoch 004: | Train Loss: 0.78467 | Val Loss: 0.99984 | Train Acc: 53.662| Val Acc: 43.875\n",
      "Epoch 005: | Train Loss: 0.77324 | Val Loss: 1.01913 | Train Acc: 54.482| Val Acc: 43.750\n",
      "Epoch 006: | Train Loss: 0.77668 | Val Loss: 0.93539 | Train Acc: 54.060| Val Acc: 46.000\n",
      "Epoch 007: | Train Loss: 0.76765 | Val Loss: 0.97366 | Train Acc: 54.476| Val Acc: 45.875\n",
      "Epoch 008: | Train Loss: 0.74609 | Val Loss: 1.00072 | Train Acc: 55.673| Val Acc: 45.375\n",
      "Epoch 009: | Train Loss: 0.74669 | Val Loss: 0.93276 | Train Acc: 55.451| Val Acc: 46.875\n",
      "Epoch 010: | Train Loss: 0.75164 | Val Loss: 0.95804 | Train Acc: 55.807| Val Acc: 47.750\n",
      "Epoch 011: | Train Loss: 0.74224 | Val Loss: 0.96554 | Train Acc: 55.507| Val Acc: 46.250\n",
      "Epoch 012: | Train Loss: 0.73771 | Val Loss: 0.94997 | Train Acc: 56.302| Val Acc: 45.875\n",
      "Epoch 013: | Train Loss: 0.75641 | Val Loss: 0.97294 | Train Acc: 55.160| Val Acc: 44.250\n",
      "Epoch 014: | Train Loss: 0.73768 | Val Loss: 0.94289 | Train Acc: 56.089| Val Acc: 46.500\n",
      "Epoch 015: | Train Loss: 0.73667 | Val Loss: 0.92282 | Train Acc: 56.244| Val Acc: 47.375\n",
      "Epoch 016: | Train Loss: 0.73294 | Val Loss: 0.96663 | Train Acc: 56.191| Val Acc: 46.000\n",
      "Epoch 017: | Train Loss: 0.72647 | Val Loss: 0.93857 | Train Acc: 56.296| Val Acc: 46.625\n",
      "Epoch 018: | Train Loss: 0.73692 | Val Loss: 0.90734 | Train Acc: 55.564| Val Acc: 48.000\n",
      "Epoch 019: | Train Loss: 0.74570 | Val Loss: 0.91448 | Train Acc: 55.316| Val Acc: 47.250\n",
      "Epoch 020: | Train Loss: 0.72965 | Val Loss: 0.96760 | Train Acc: 56.711| Val Acc: 45.125\n",
      "Epoch 021: | Train Loss: 0.71680 | Val Loss: 0.92641 | Train Acc: 56.784| Val Acc: 46.125\n",
      "Epoch 022: | Train Loss: 0.71535 | Val Loss: 0.94236 | Train Acc: 57.240| Val Acc: 47.000\n",
      "Epoch 023: | Train Loss: 0.72774 | Val Loss: 0.93883 | Train Acc: 56.189| Val Acc: 46.750\n",
      "Epoch 024: | Train Loss: 0.73643 | Val Loss: 0.94199 | Train Acc: 56.500| Val Acc: 46.500\n",
      "Epoch 025: | Train Loss: 0.71346 | Val Loss: 0.90321 | Train Acc: 57.033| Val Acc: 49.125\n",
      "Epoch 026: | Train Loss: 0.73746 | Val Loss: 0.90268 | Train Acc: 55.416| Val Acc: 47.875\n",
      "Epoch 027: | Train Loss: 0.72130 | Val Loss: 0.95249 | Train Acc: 56.444| Val Acc: 46.500\n",
      "Epoch 028: | Train Loss: 0.72303 | Val Loss: 0.92222 | Train Acc: 56.518| Val Acc: 48.500\n",
      "Epoch 029: | Train Loss: 0.71629 | Val Loss: 0.92378 | Train Acc: 57.022| Val Acc: 48.000\n",
      "Epoch 030: | Train Loss: 0.72651 | Val Loss: 0.96391 | Train Acc: 56.851| Val Acc: 45.375\n",
      "Epoch 031: | Train Loss: 0.73630 | Val Loss: 0.91118 | Train Acc: 56.211| Val Acc: 49.250\n",
      "Epoch 032: | Train Loss: 0.70205 | Val Loss: 0.94067 | Train Acc: 57.924| Val Acc: 47.125\n",
      "Epoch 033: | Train Loss: 0.70082 | Val Loss: 0.93133 | Train Acc: 57.658| Val Acc: 48.875\n",
      "Epoch 034: | Train Loss: 0.71800 | Val Loss: 0.90201 | Train Acc: 56.920| Val Acc: 48.875\n",
      "Epoch 035: | Train Loss: 0.69999 | Val Loss: 0.92282 | Train Acc: 57.902| Val Acc: 47.375\n",
      "Epoch 036: | Train Loss: 0.69511 | Val Loss: 0.92664 | Train Acc: 58.322| Val Acc: 48.750\n",
      "Epoch 037: | Train Loss: 0.70017 | Val Loss: 0.90949 | Train Acc: 57.978| Val Acc: 48.375\n",
      "Epoch 038: | Train Loss: 0.70822 | Val Loss: 0.91411 | Train Acc: 57.618| Val Acc: 48.375\n",
      "Epoch 039: | Train Loss: 0.71169 | Val Loss: 0.91626 | Train Acc: 56.709| Val Acc: 47.125\n",
      "Epoch 040: | Train Loss: 0.69215 | Val Loss: 0.90150 | Train Acc: 58.244| Val Acc: 48.500\n",
      "Epoch 041: | Train Loss: 0.70454 | Val Loss: 0.91355 | Train Acc: 57.722| Val Acc: 48.000\n",
      "Epoch 042: | Train Loss: 0.69280 | Val Loss: 0.92632 | Train Acc: 58.287| Val Acc: 48.125\n",
      "Epoch 043: | Train Loss: 0.70091 | Val Loss: 0.91529 | Train Acc: 57.867| Val Acc: 47.750\n",
      "Epoch 044: | Train Loss: 0.68964 | Val Loss: 0.91320 | Train Acc: 57.936| Val Acc: 48.125\n",
      "Epoch 045: | Train Loss: 0.70233 | Val Loss: 0.90804 | Train Acc: 57.240| Val Acc: 47.375\n",
      "Epoch 046: | Train Loss: 0.70349 | Val Loss: 0.88263 | Train Acc: 57.827| Val Acc: 49.750\n",
      "Epoch 047: | Train Loss: 0.68647 | Val Loss: 0.89134 | Train Acc: 58.127| Val Acc: 49.500\n",
      "Epoch 048: | Train Loss: 0.70357 | Val Loss: 0.89278 | Train Acc: 57.929| Val Acc: 48.375\n",
      "Epoch 049: | Train Loss: 0.68783 | Val Loss: 0.90838 | Train Acc: 58.660| Val Acc: 48.000\n",
      "Epoch 050: | Train Loss: 0.70155 | Val Loss: 0.90439 | Train Acc: 57.604| Val Acc: 49.625\n",
      "Epoch 051: | Train Loss: 0.68505 | Val Loss: 0.92410 | Train Acc: 59.371| Val Acc: 48.500\n",
      "Epoch 052: | Train Loss: 0.68888 | Val Loss: 0.91939 | Train Acc: 58.707| Val Acc: 48.125\n",
      "Epoch 053: | Train Loss: 0.68697 | Val Loss: 0.92092 | Train Acc: 57.689| Val Acc: 48.500\n",
      "Epoch 054: | Train Loss: 0.70031 | Val Loss: 0.89262 | Train Acc: 57.482| Val Acc: 48.750\n",
      "Epoch 055: | Train Loss: 0.68695 | Val Loss: 0.92346 | Train Acc: 58.820| Val Acc: 47.750\n",
      "Epoch 056: | Train Loss: 0.68259 | Val Loss: 0.94992 | Train Acc: 58.760| Val Acc: 47.000\n",
      "Epoch 057: | Train Loss: 0.68007 | Val Loss: 0.94568 | Train Acc: 58.964| Val Acc: 47.250\n",
      "Epoch 058: | Train Loss: 0.68638 | Val Loss: 0.89075 | Train Acc: 57.853| Val Acc: 48.750\n",
      "Epoch 059: | Train Loss: 0.68110 | Val Loss: 0.92977 | Train Acc: 59.047| Val Acc: 46.375\n",
      "Epoch 060: | Train Loss: 0.69913 | Val Loss: 0.92809 | Train Acc: 57.711| Val Acc: 47.375\n",
      "Epoch 061: | Train Loss: 0.68918 | Val Loss: 0.89542 | Train Acc: 58.682| Val Acc: 49.125\n",
      "Epoch 062: | Train Loss: 0.68980 | Val Loss: 0.90589 | Train Acc: 57.844| Val Acc: 49.625\n",
      "Epoch 063: | Train Loss: 0.68269 | Val Loss: 0.93222 | Train Acc: 59.071| Val Acc: 48.125\n",
      "Epoch 064: | Train Loss: 0.67508 | Val Loss: 0.89797 | Train Acc: 58.364| Val Acc: 50.250\n",
      "Epoch 065: | Train Loss: 0.67023 | Val Loss: 0.91924 | Train Acc: 59.784| Val Acc: 48.125\n",
      "Epoch 066: | Train Loss: 0.67682 | Val Loss: 0.95845 | Train Acc: 59.700| Val Acc: 46.750\n",
      "Epoch 067: | Train Loss: 0.68775 | Val Loss: 0.92771 | Train Acc: 58.420| Val Acc: 48.000\n",
      "Epoch 068: | Train Loss: 0.68207 | Val Loss: 0.93200 | Train Acc: 59.018| Val Acc: 48.500\n",
      "Epoch 069: | Train Loss: 0.69401 | Val Loss: 0.89184 | Train Acc: 58.056| Val Acc: 49.875\n",
      "Epoch 070: | Train Loss: 0.69097 | Val Loss: 0.87642 | Train Acc: 58.373| Val Acc: 50.375\n",
      "Epoch 071: | Train Loss: 0.67681 | Val Loss: 0.91594 | Train Acc: 59.738| Val Acc: 49.750\n",
      "Epoch 072: | Train Loss: 0.69732 | Val Loss: 0.89881 | Train Acc: 57.924| Val Acc: 49.000\n",
      "Epoch 073: | Train Loss: 0.68620 | Val Loss: 0.90805 | Train Acc: 58.902| Val Acc: 50.125\n",
      "Epoch 074: | Train Loss: 0.67884 | Val Loss: 0.91677 | Train Acc: 58.731| Val Acc: 47.375\n",
      "Epoch 075: | Train Loss: 0.68728 | Val Loss: 0.90928 | Train Acc: 57.956| Val Acc: 49.250\n",
      "Epoch 076: | Train Loss: 0.68063 | Val Loss: 0.89846 | Train Acc: 58.193| Val Acc: 49.125\n",
      "Epoch 077: | Train Loss: 0.69628 | Val Loss: 0.91326 | Train Acc: 58.511| Val Acc: 47.875\n",
      "Epoch 078: | Train Loss: 0.67308 | Val Loss: 0.91616 | Train Acc: 58.876| Val Acc: 47.375\n",
      "Epoch 079: | Train Loss: 0.68281 | Val Loss: 0.86336 | Train Acc: 58.380| Val Acc: 50.875\n",
      "Epoch 080: | Train Loss: 0.67128 | Val Loss: 0.90353 | Train Acc: 59.513| Val Acc: 49.000\n",
      "Epoch 081: | Train Loss: 0.67371 | Val Loss: 0.91379 | Train Acc: 59.196| Val Acc: 49.500\n",
      "Epoch 082: | Train Loss: 0.66493 | Val Loss: 0.92986 | Train Acc: 59.036| Val Acc: 47.625\n",
      "Epoch 083: | Train Loss: 0.67384 | Val Loss: 0.92760 | Train Acc: 58.784| Val Acc: 47.875\n",
      "Epoch 084: | Train Loss: 0.68732 | Val Loss: 0.91659 | Train Acc: 58.078| Val Acc: 48.000\n",
      "Epoch 085: | Train Loss: 0.67808 | Val Loss: 0.92731 | Train Acc: 58.851| Val Acc: 48.125\n",
      "Epoch 086: | Train Loss: 0.66862 | Val Loss: 0.89659 | Train Acc: 60.124| Val Acc: 49.250\n",
      "Epoch 087: | Train Loss: 0.69322 | Val Loss: 0.88488 | Train Acc: 58.038| Val Acc: 48.875\n",
      "Epoch 088: | Train Loss: 0.67804 | Val Loss: 0.90303 | Train Acc: 59.202| Val Acc: 49.750\n",
      "Epoch 089: | Train Loss: 0.67530 | Val Loss: 0.94610 | Train Acc: 59.218| Val Acc: 47.625\n",
      "Epoch 090: | Train Loss: 0.67352 | Val Loss: 0.92446 | Train Acc: 58.464| Val Acc: 47.625\n",
      "Epoch 091: | Train Loss: 0.67799 | Val Loss: 0.89519 | Train Acc: 58.678| Val Acc: 49.500\n",
      "Epoch 092: | Train Loss: 0.67672 | Val Loss: 0.89561 | Train Acc: 58.653| Val Acc: 49.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 093: | Train Loss: 0.67268 | Val Loss: 0.88049 | Train Acc: 58.064| Val Acc: 50.250\n",
      "Epoch 094: | Train Loss: 0.67802 | Val Loss: 0.91282 | Train Acc: 58.811| Val Acc: 48.500\n",
      "Epoch 095: | Train Loss: 0.68423 | Val Loss: 0.90057 | Train Acc: 58.582| Val Acc: 49.000\n",
      "Epoch 096: | Train Loss: 0.67548 | Val Loss: 0.89646 | Train Acc: 58.296| Val Acc: 48.875\n",
      "Epoch 097: | Train Loss: 0.66432 | Val Loss: 0.91533 | Train Acc: 59.547| Val Acc: 49.625\n",
      "Epoch 098: | Train Loss: 0.68135 | Val Loss: 0.90043 | Train Acc: 58.233| Val Acc: 49.375\n",
      "Epoch 099: | Train Loss: 0.68076 | Val Loss: 0.88975 | Train Acc: 58.202| Val Acc: 49.250\n",
      "Epoch 100: | Train Loss: 0.67271 | Val Loss: 0.90561 | Train Acc: 58.956| Val Acc: 50.125\n",
      "Epoch 101: | Train Loss: 0.67430 | Val Loss: 0.88535 | Train Acc: 58.296| Val Acc: 49.750\n",
      "Epoch 102: | Train Loss: 0.66216 | Val Loss: 0.91781 | Train Acc: 60.007| Val Acc: 48.875\n",
      "Epoch 103: | Train Loss: 0.69146 | Val Loss: 0.92730 | Train Acc: 58.162| Val Acc: 49.125\n",
      "Epoch 104: | Train Loss: 0.67299 | Val Loss: 0.88899 | Train Acc: 58.809| Val Acc: 48.500\n",
      "Epoch 105: | Train Loss: 0.66963 | Val Loss: 0.88179 | Train Acc: 59.267| Val Acc: 50.250\n",
      "Epoch 106: | Train Loss: 0.67838 | Val Loss: 0.89159 | Train Acc: 58.149| Val Acc: 50.125\n",
      "Epoch 107: | Train Loss: 0.67304 | Val Loss: 0.90014 | Train Acc: 58.649| Val Acc: 49.625\n",
      "Epoch 108: | Train Loss: 0.65977 | Val Loss: 0.91941 | Train Acc: 59.876| Val Acc: 47.500\n",
      "Epoch 109: | Train Loss: 0.67841 | Val Loss: 0.91143 | Train Acc: 58.971| Val Acc: 48.500\n",
      "Epoch 110: | Train Loss: 0.66216 | Val Loss: 0.90631 | Train Acc: 59.184| Val Acc: 49.875\n",
      "Epoch 111: | Train Loss: 0.66202 | Val Loss: 0.88352 | Train Acc: 59.169| Val Acc: 50.000\n",
      "Epoch 112: | Train Loss: 0.67034 | Val Loss: 0.88989 | Train Acc: 58.882| Val Acc: 50.375\n",
      "Epoch 113: | Train Loss: 0.66239 | Val Loss: 0.88114 | Train Acc: 59.336| Val Acc: 49.125\n",
      "Epoch 114: | Train Loss: 0.67702 | Val Loss: 0.90897 | Train Acc: 58.800| Val Acc: 48.750\n",
      "Epoch 115: | Train Loss: 0.65617 | Val Loss: 0.90704 | Train Acc: 59.780| Val Acc: 49.125\n",
      "Epoch 116: | Train Loss: 0.66856 | Val Loss: 0.89800 | Train Acc: 58.673| Val Acc: 49.125\n",
      "Epoch 117: | Train Loss: 0.67618 | Val Loss: 0.91290 | Train Acc: 58.700| Val Acc: 48.250\n",
      "Epoch 118: | Train Loss: 0.64957 | Val Loss: 0.91116 | Train Acc: 60.149| Val Acc: 48.375\n",
      "Epoch 119: | Train Loss: 0.66951 | Val Loss: 0.90793 | Train Acc: 59.713| Val Acc: 49.375\n",
      "Epoch 120: | Train Loss: 0.66604 | Val Loss: 0.91867 | Train Acc: 59.409| Val Acc: 49.625\n",
      "Epoch 121: | Train Loss: 0.65947 | Val Loss: 0.90129 | Train Acc: 59.178| Val Acc: 48.625\n",
      "Epoch 122: | Train Loss: 0.67412 | Val Loss: 0.91325 | Train Acc: 58.627| Val Acc: 49.625\n",
      "Epoch 123: | Train Loss: 0.66890 | Val Loss: 0.88374 | Train Acc: 59.116| Val Acc: 49.250\n",
      "Epoch 124: | Train Loss: 0.64527 | Val Loss: 0.87749 | Train Acc: 60.089| Val Acc: 50.250\n",
      "Epoch 125: | Train Loss: 0.65789 | Val Loss: 0.89876 | Train Acc: 60.589| Val Acc: 49.375\n",
      "Epoch 126: | Train Loss: 0.67162 | Val Loss: 0.92656 | Train Acc: 58.580| Val Acc: 48.250\n",
      "Epoch 127: | Train Loss: 0.67963 | Val Loss: 0.91756 | Train Acc: 58.711| Val Acc: 48.625\n",
      "Epoch 128: | Train Loss: 0.65195 | Val Loss: 0.93259 | Train Acc: 59.302| Val Acc: 47.250\n",
      "Epoch 129: | Train Loss: 0.66373 | Val Loss: 0.90820 | Train Acc: 59.533| Val Acc: 48.625\n",
      "Epoch 130: | Train Loss: 0.66448 | Val Loss: 0.89251 | Train Acc: 59.329| Val Acc: 48.625\n",
      "Epoch 131: | Train Loss: 0.65783 | Val Loss: 0.87992 | Train Acc: 59.396| Val Acc: 49.750\n",
      "Epoch 132: | Train Loss: 0.68465 | Val Loss: 0.88683 | Train Acc: 58.113| Val Acc: 48.750\n",
      "Epoch 133: | Train Loss: 0.65850 | Val Loss: 0.89108 | Train Acc: 59.544| Val Acc: 49.750\n",
      "Epoch 134: | Train Loss: 0.67171 | Val Loss: 0.89760 | Train Acc: 58.351| Val Acc: 48.500\n",
      "Epoch 135: | Train Loss: 0.66222 | Val Loss: 0.88905 | Train Acc: 59.311| Val Acc: 49.875\n",
      "Epoch 136: | Train Loss: 0.66570 | Val Loss: 0.94191 | Train Acc: 59.858| Val Acc: 47.500\n",
      "Epoch 137: | Train Loss: 0.68599 | Val Loss: 0.89671 | Train Acc: 58.064| Val Acc: 48.250\n",
      "Epoch 138: | Train Loss: 0.65632 | Val Loss: 0.88809 | Train Acc: 59.436| Val Acc: 49.750\n",
      "Epoch 139: | Train Loss: 0.66629 | Val Loss: 0.91155 | Train Acc: 59.584| Val Acc: 48.500\n",
      "Epoch 140: | Train Loss: 0.65758 | Val Loss: 0.90786 | Train Acc: 60.351| Val Acc: 49.500\n",
      "Epoch 141: | Train Loss: 0.67391 | Val Loss: 0.89241 | Train Acc: 59.162| Val Acc: 49.875\n",
      "Epoch 142: | Train Loss: 0.66131 | Val Loss: 0.90378 | Train Acc: 59.887| Val Acc: 49.500\n",
      "Epoch 143: | Train Loss: 0.65093 | Val Loss: 0.89980 | Train Acc: 60.251| Val Acc: 49.000\n",
      "Epoch 144: | Train Loss: 0.67215 | Val Loss: 0.89553 | Train Acc: 59.022| Val Acc: 48.125\n",
      "Epoch 145: | Train Loss: 0.65867 | Val Loss: 0.89912 | Train Acc: 59.818| Val Acc: 48.875\n",
      "Epoch 146: | Train Loss: 0.67108 | Val Loss: 0.89824 | Train Acc: 59.022| Val Acc: 48.250\n",
      "Epoch 147: | Train Loss: 0.67316 | Val Loss: 0.91778 | Train Acc: 58.422| Val Acc: 47.625\n",
      "Epoch 148: | Train Loss: 0.68413 | Val Loss: 0.89088 | Train Acc: 57.489| Val Acc: 48.500\n",
      "Epoch 149: | Train Loss: 0.67557 | Val Loss: 0.91137 | Train Acc: 58.413| Val Acc: 49.250\n",
      "Epoch 150: | Train Loss: 0.68011 | Val Loss: 0.88702 | Train Acc: 58.444| Val Acc: 48.875\n",
      "Epoch 151: | Train Loss: 0.66856 | Val Loss: 0.95287 | Train Acc: 59.291| Val Acc: 46.875\n",
      "Epoch 152: | Train Loss: 0.66887 | Val Loss: 0.89256 | Train Acc: 59.107| Val Acc: 49.500\n",
      "Epoch 153: | Train Loss: 0.67399 | Val Loss: 0.90787 | Train Acc: 59.027| Val Acc: 48.625\n",
      "Epoch 154: | Train Loss: 0.65928 | Val Loss: 0.93948 | Train Acc: 59.304| Val Acc: 47.375\n",
      "Epoch 155: | Train Loss: 0.66681 | Val Loss: 0.93890 | Train Acc: 60.062| Val Acc: 47.500\n",
      "Epoch 156: | Train Loss: 0.64729 | Val Loss: 0.91335 | Train Acc: 59.740| Val Acc: 48.000\n",
      "Epoch 157: | Train Loss: 0.67573 | Val Loss: 0.89648 | Train Acc: 58.327| Val Acc: 49.000\n",
      "Epoch 158: | Train Loss: 0.65367 | Val Loss: 0.89925 | Train Acc: 59.618| Val Acc: 48.750\n",
      "Epoch 159: | Train Loss: 0.65958 | Val Loss: 0.89722 | Train Acc: 59.469| Val Acc: 49.000\n",
      "Epoch 160: | Train Loss: 0.67854 | Val Loss: 0.91182 | Train Acc: 57.900| Val Acc: 48.250\n",
      "Epoch 161: | Train Loss: 0.65070 | Val Loss: 0.91625 | Train Acc: 59.707| Val Acc: 48.875\n",
      "Epoch 162: | Train Loss: 0.66375 | Val Loss: 0.88971 | Train Acc: 59.522| Val Acc: 50.125\n",
      "Epoch 163: | Train Loss: 0.67011 | Val Loss: 0.88818 | Train Acc: 58.502| Val Acc: 49.875\n",
      "Epoch 164: | Train Loss: 0.66381 | Val Loss: 0.91003 | Train Acc: 59.651| Val Acc: 49.000\n",
      "Epoch 165: | Train Loss: 0.66205 | Val Loss: 0.91581 | Train Acc: 59.687| Val Acc: 48.750\n",
      "Epoch 166: | Train Loss: 0.65865 | Val Loss: 0.88002 | Train Acc: 59.602| Val Acc: 50.875\n",
      "Epoch 167: | Train Loss: 0.67280 | Val Loss: 0.91686 | Train Acc: 59.564| Val Acc: 49.125\n",
      "Epoch 168: | Train Loss: 0.66648 | Val Loss: 0.90364 | Train Acc: 58.378| Val Acc: 48.625\n",
      "Epoch 169: | Train Loss: 0.65275 | Val Loss: 0.90520 | Train Acc: 59.709| Val Acc: 49.375\n",
      "Epoch 170: | Train Loss: 0.65623 | Val Loss: 0.89205 | Train Acc: 59.682| Val Acc: 48.625\n",
      "Epoch 171: | Train Loss: 0.65649 | Val Loss: 0.89921 | Train Acc: 58.902| Val Acc: 49.250\n",
      "Epoch 172: | Train Loss: 0.67023 | Val Loss: 0.92499 | Train Acc: 58.816| Val Acc: 48.375\n",
      "Epoch 173: | Train Loss: 0.66676 | Val Loss: 0.89194 | Train Acc: 58.711| Val Acc: 49.500\n",
      "Epoch 174: | Train Loss: 0.66616 | Val Loss: 0.92048 | Train Acc: 58.896| Val Acc: 49.500\n",
      "Epoch 175: | Train Loss: 0.64523 | Val Loss: 0.90143 | Train Acc: 60.416| Val Acc: 49.500\n",
      "Epoch 176: | Train Loss: 0.67152 | Val Loss: 0.90367 | Train Acc: 58.664| Val Acc: 49.125\n",
      "Epoch 177: | Train Loss: 0.66243 | Val Loss: 0.90807 | Train Acc: 59.127| Val Acc: 48.375\n",
      "Epoch 178: | Train Loss: 0.66260 | Val Loss: 0.91068 | Train Acc: 59.451| Val Acc: 47.125\n",
      "Epoch 179: | Train Loss: 0.67016 | Val Loss: 0.88157 | Train Acc: 58.620| Val Acc: 49.375\n",
      "Epoch 180: | Train Loss: 0.66263 | Val Loss: 0.90190 | Train Acc: 60.033| Val Acc: 49.125\n",
      "Epoch 181: | Train Loss: 0.65919 | Val Loss: 0.91187 | Train Acc: 59.118| Val Acc: 47.625\n",
      "Epoch 182: | Train Loss: 0.66644 | Val Loss: 0.91844 | Train Acc: 59.218| Val Acc: 48.875\n",
      "Epoch 183: | Train Loss: 0.66512 | Val Loss: 0.88989 | Train Acc: 59.336| Val Acc: 48.625\n",
      "Epoch 184: | Train Loss: 0.66005 | Val Loss: 0.89116 | Train Acc: 59.460| Val Acc: 49.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185: | Train Loss: 0.65357 | Val Loss: 0.87574 | Train Acc: 59.920| Val Acc: 50.375\n",
      "Epoch 186: | Train Loss: 0.66437 | Val Loss: 0.97140 | Train Acc: 59.871| Val Acc: 46.750\n",
      "Epoch 187: | Train Loss: 0.66391 | Val Loss: 0.89030 | Train Acc: 59.024| Val Acc: 48.875\n",
      "Epoch 188: | Train Loss: 0.68344 | Val Loss: 0.90549 | Train Acc: 57.787| Val Acc: 48.375\n",
      "Epoch 189: | Train Loss: 0.66226 | Val Loss: 0.95354 | Train Acc: 59.524| Val Acc: 46.625\n",
      "Epoch 190: | Train Loss: 0.65906 | Val Loss: 0.90834 | Train Acc: 59.922| Val Acc: 49.250\n",
      "Epoch 191: | Train Loss: 0.65095 | Val Loss: 0.90049 | Train Acc: 59.324| Val Acc: 50.125\n",
      "Epoch 192: | Train Loss: 0.66736 | Val Loss: 0.93275 | Train Acc: 58.796| Val Acc: 49.500\n",
      "Epoch 193: | Train Loss: 0.64491 | Val Loss: 0.90310 | Train Acc: 60.253| Val Acc: 49.750\n",
      "Epoch 194: | Train Loss: 0.66277 | Val Loss: 0.89640 | Train Acc: 59.378| Val Acc: 50.625\n",
      "Epoch 195: | Train Loss: 0.65675 | Val Loss: 0.93938 | Train Acc: 60.167| Val Acc: 49.500\n",
      "Epoch 196: | Train Loss: 0.65815 | Val Loss: 0.88669 | Train Acc: 59.056| Val Acc: 50.375\n",
      "Epoch 197: | Train Loss: 0.65606 | Val Loss: 0.89447 | Train Acc: 59.898| Val Acc: 49.000\n",
      "Epoch 198: | Train Loss: 0.65932 | Val Loss: 0.90597 | Train Acc: 59.507| Val Acc: 48.625\n",
      "Epoch 199: | Train Loss: 0.65747 | Val Loss: 0.96866 | Train Acc: 59.698| Val Acc: 47.875\n",
      "Epoch 200: | Train Loss: 0.65271 | Val Loss: 0.89676 | Train Acc: 59.593| Val Acc: 49.750\n",
      "Epoch 201: | Train Loss: 0.66730 | Val Loss: 0.91391 | Train Acc: 58.980| Val Acc: 49.250\n",
      "Epoch 202: | Train Loss: 0.65862 | Val Loss: 0.90296 | Train Acc: 59.482| Val Acc: 49.625\n",
      "Epoch 203: | Train Loss: 0.67535 | Val Loss: 0.89593 | Train Acc: 58.776| Val Acc: 48.750\n",
      "Epoch 204: | Train Loss: 0.64534 | Val Loss: 0.88583 | Train Acc: 60.204| Val Acc: 49.750\n",
      "Epoch 205: | Train Loss: 0.65371 | Val Loss: 0.88587 | Train Acc: 60.020| Val Acc: 49.375\n",
      "Epoch 206: | Train Loss: 0.66082 | Val Loss: 0.90255 | Train Acc: 59.613| Val Acc: 50.000\n",
      "Epoch 207: | Train Loss: 0.66001 | Val Loss: 0.92428 | Train Acc: 59.269| Val Acc: 48.625\n",
      "Epoch 208: | Train Loss: 0.66990 | Val Loss: 0.91056 | Train Acc: 59.880| Val Acc: 49.000\n",
      "Epoch 209: | Train Loss: 0.66249 | Val Loss: 0.90287 | Train Acc: 60.051| Val Acc: 48.625\n",
      "Epoch 210: | Train Loss: 0.66161 | Val Loss: 0.91459 | Train Acc: 59.118| Val Acc: 48.625\n",
      "Epoch 211: | Train Loss: 0.65450 | Val Loss: 0.86717 | Train Acc: 59.569| Val Acc: 50.250\n",
      "Epoch 212: | Train Loss: 0.65364 | Val Loss: 0.89993 | Train Acc: 59.729| Val Acc: 49.625\n",
      "Epoch 213: | Train Loss: 0.65309 | Val Loss: 0.92910 | Train Acc: 59.724| Val Acc: 48.125\n",
      "Epoch 214: | Train Loss: 0.66711 | Val Loss: 0.89961 | Train Acc: 59.798| Val Acc: 50.125\n",
      "Epoch 215: | Train Loss: 0.65529 | Val Loss: 0.88596 | Train Acc: 59.820| Val Acc: 49.000\n",
      "Epoch 216: | Train Loss: 0.66524 | Val Loss: 0.88028 | Train Acc: 58.811| Val Acc: 49.000\n",
      "Epoch 217: | Train Loss: 0.64889 | Val Loss: 0.91232 | Train Acc: 60.087| Val Acc: 49.250\n",
      "Epoch 218: | Train Loss: 0.67152 | Val Loss: 0.88590 | Train Acc: 58.844| Val Acc: 49.625\n",
      "Epoch 219: | Train Loss: 0.66175 | Val Loss: 0.90901 | Train Acc: 59.127| Val Acc: 48.250\n",
      "Epoch 220: | Train Loss: 0.65698 | Val Loss: 0.90116 | Train Acc: 59.082| Val Acc: 49.750\n",
      "Epoch 221: | Train Loss: 0.65292 | Val Loss: 0.91394 | Train Acc: 60.224| Val Acc: 49.875\n",
      "Epoch 222: | Train Loss: 0.67368 | Val Loss: 0.91604 | Train Acc: 58.218| Val Acc: 48.625\n",
      "Epoch 223: | Train Loss: 0.65299 | Val Loss: 0.94926 | Train Acc: 60.178| Val Acc: 49.250\n",
      "Epoch 224: | Train Loss: 0.64181 | Val Loss: 0.89047 | Train Acc: 60.822| Val Acc: 50.000\n",
      "Epoch 225: | Train Loss: 0.66929 | Val Loss: 0.92963 | Train Acc: 59.740| Val Acc: 48.625\n",
      "Epoch 226: | Train Loss: 0.66052 | Val Loss: 0.90659 | Train Acc: 59.447| Val Acc: 49.375\n",
      "Epoch 227: | Train Loss: 0.66562 | Val Loss: 0.92928 | Train Acc: 59.227| Val Acc: 47.875\n",
      "Epoch 228: | Train Loss: 0.66193 | Val Loss: 0.91148 | Train Acc: 59.938| Val Acc: 49.625\n",
      "Epoch 229: | Train Loss: 0.65107 | Val Loss: 0.90447 | Train Acc: 59.607| Val Acc: 49.125\n",
      "Epoch 230: | Train Loss: 0.66284 | Val Loss: 0.89039 | Train Acc: 59.124| Val Acc: 49.125\n",
      "Epoch 231: | Train Loss: 0.65749 | Val Loss: 0.89627 | Train Acc: 59.427| Val Acc: 49.125\n",
      "Epoch 232: | Train Loss: 0.64961 | Val Loss: 0.90547 | Train Acc: 60.340| Val Acc: 49.500\n",
      "Epoch 233: | Train Loss: 0.65219 | Val Loss: 0.89384 | Train Acc: 59.976| Val Acc: 49.875\n",
      "Epoch 234: | Train Loss: 0.66062 | Val Loss: 0.90938 | Train Acc: 59.347| Val Acc: 48.500\n",
      "Epoch 235: | Train Loss: 0.65862 | Val Loss: 0.90951 | Train Acc: 59.469| Val Acc: 49.375\n",
      "Epoch 236: | Train Loss: 0.66716 | Val Loss: 0.91367 | Train Acc: 58.278| Val Acc: 49.125\n",
      "Epoch 237: | Train Loss: 0.65271 | Val Loss: 0.89005 | Train Acc: 59.196| Val Acc: 50.000\n",
      "Epoch 238: | Train Loss: 0.66793 | Val Loss: 0.91413 | Train Acc: 59.547| Val Acc: 49.125\n",
      "Epoch 239: | Train Loss: 0.65788 | Val Loss: 0.94394 | Train Acc: 60.389| Val Acc: 48.000\n",
      "Epoch 240: | Train Loss: 0.64664 | Val Loss: 0.90273 | Train Acc: 59.689| Val Acc: 48.500\n",
      "Epoch 241: | Train Loss: 0.65417 | Val Loss: 0.96835 | Train Acc: 60.251| Val Acc: 46.875\n",
      "Epoch 242: | Train Loss: 0.65976 | Val Loss: 0.92150 | Train Acc: 59.631| Val Acc: 50.375\n",
      "Epoch 243: | Train Loss: 0.66051 | Val Loss: 0.89503 | Train Acc: 60.131| Val Acc: 49.500\n",
      "Epoch 244: | Train Loss: 0.65882 | Val Loss: 0.92545 | Train Acc: 59.024| Val Acc: 48.250\n",
      "Epoch 245: | Train Loss: 0.65702 | Val Loss: 0.90159 | Train Acc: 59.396| Val Acc: 48.875\n",
      "Epoch 246: | Train Loss: 0.65364 | Val Loss: 0.90009 | Train Acc: 59.536| Val Acc: 48.250\n",
      "Epoch 247: | Train Loss: 0.65506 | Val Loss: 0.98621 | Train Acc: 60.027| Val Acc: 47.625\n",
      "Epoch 248: | Train Loss: 0.64463 | Val Loss: 0.88764 | Train Acc: 59.789| Val Acc: 50.250\n",
      "Epoch 249: | Train Loss: 0.65339 | Val Loss: 0.92611 | Train Acc: 59.636| Val Acc: 48.875\n",
      "Epoch 250: | Train Loss: 0.66368 | Val Loss: 0.91488 | Train Acc: 59.278| Val Acc: 48.750\n",
      "Epoch 251: | Train Loss: 0.64471 | Val Loss: 0.90724 | Train Acc: 60.104| Val Acc: 48.625\n",
      "Epoch 252: | Train Loss: 0.64591 | Val Loss: 0.90057 | Train Acc: 60.167| Val Acc: 49.625\n",
      "Epoch 253: | Train Loss: 0.65203 | Val Loss: 0.91155 | Train Acc: 59.564| Val Acc: 48.500\n",
      "Epoch 254: | Train Loss: 0.64211 | Val Loss: 0.89589 | Train Acc: 60.502| Val Acc: 49.375\n",
      "Epoch 255: | Train Loss: 0.65880 | Val Loss: 0.90467 | Train Acc: 59.458| Val Acc: 49.250\n",
      "Epoch 256: | Train Loss: 0.66449 | Val Loss: 0.91365 | Train Acc: 59.329| Val Acc: 48.500\n",
      "Epoch 257: | Train Loss: 0.64873 | Val Loss: 0.90141 | Train Acc: 60.622| Val Acc: 49.625\n",
      "Epoch 258: | Train Loss: 0.66991 | Val Loss: 0.88903 | Train Acc: 58.933| Val Acc: 50.000\n",
      "Epoch 259: | Train Loss: 0.64467 | Val Loss: 0.95759 | Train Acc: 59.673| Val Acc: 48.000\n",
      "Epoch 260: | Train Loss: 0.64751 | Val Loss: 0.90696 | Train Acc: 59.922| Val Acc: 50.375\n",
      "Epoch 261: | Train Loss: 0.65388 | Val Loss: 0.90844 | Train Acc: 60.107| Val Acc: 48.625\n",
      "Epoch 262: | Train Loss: 0.66821 | Val Loss: 0.89759 | Train Acc: 58.967| Val Acc: 50.250\n",
      "Epoch 263: | Train Loss: 0.65902 | Val Loss: 0.91734 | Train Acc: 59.376| Val Acc: 49.125\n",
      "Epoch 264: | Train Loss: 0.65793 | Val Loss: 0.91217 | Train Acc: 59.493| Val Acc: 48.500\n",
      "Epoch 265: | Train Loss: 0.65906 | Val Loss: 0.94654 | Train Acc: 58.891| Val Acc: 48.500\n",
      "Epoch 266: | Train Loss: 0.65506 | Val Loss: 0.89868 | Train Acc: 59.453| Val Acc: 49.375\n",
      "Epoch 267: | Train Loss: 0.65362 | Val Loss: 0.91441 | Train Acc: 59.793| Val Acc: 49.250\n",
      "Epoch 268: | Train Loss: 0.65447 | Val Loss: 0.96194 | Train Acc: 59.382| Val Acc: 48.250\n",
      "Epoch 269: | Train Loss: 0.65851 | Val Loss: 0.91774 | Train Acc: 59.998| Val Acc: 48.625\n",
      "Epoch 270: | Train Loss: 0.66157 | Val Loss: 0.90206 | Train Acc: 58.580| Val Acc: 49.875\n",
      "Epoch 271: | Train Loss: 0.66021 | Val Loss: 0.91318 | Train Acc: 59.031| Val Acc: 48.375\n",
      "Epoch 272: | Train Loss: 0.66585 | Val Loss: 0.92033 | Train Acc: 59.078| Val Acc: 48.625\n",
      "Epoch 273: | Train Loss: 0.65458 | Val Loss: 0.92580 | Train Acc: 59.653| Val Acc: 50.750\n",
      "Epoch 274: | Train Loss: 0.64384 | Val Loss: 0.88890 | Train Acc: 60.564| Val Acc: 48.625\n",
      "Epoch 275: | Train Loss: 0.65526 | Val Loss: 0.91670 | Train Acc: 59.093| Val Acc: 47.750\n",
      "Epoch 276: | Train Loss: 0.66513 | Val Loss: 0.90728 | Train Acc: 58.718| Val Acc: 48.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277: | Train Loss: 0.66067 | Val Loss: 0.90150 | Train Acc: 59.591| Val Acc: 48.375\n",
      "Epoch 278: | Train Loss: 0.65100 | Val Loss: 0.90308 | Train Acc: 59.809| Val Acc: 48.375\n",
      "Epoch 279: | Train Loss: 0.65059 | Val Loss: 0.92179 | Train Acc: 59.482| Val Acc: 48.625\n",
      "Epoch 280: | Train Loss: 0.64721 | Val Loss: 0.90926 | Train Acc: 60.196| Val Acc: 49.750\n",
      "Epoch 281: | Train Loss: 0.63391 | Val Loss: 0.92239 | Train Acc: 60.416| Val Acc: 49.750\n",
      "Epoch 282: | Train Loss: 0.64890 | Val Loss: 0.94679 | Train Acc: 60.371| Val Acc: 48.125\n",
      "Epoch 283: | Train Loss: 0.67303 | Val Loss: 0.91454 | Train Acc: 59.022| Val Acc: 49.375\n",
      "Epoch 284: | Train Loss: 0.66537 | Val Loss: 0.91742 | Train Acc: 59.371| Val Acc: 48.000\n",
      "Epoch 285: | Train Loss: 0.64777 | Val Loss: 0.95657 | Train Acc: 59.462| Val Acc: 47.500\n",
      "Epoch 286: | Train Loss: 0.64310 | Val Loss: 0.90709 | Train Acc: 60.573| Val Acc: 49.250\n",
      "Epoch 287: | Train Loss: 0.66075 | Val Loss: 0.93704 | Train Acc: 59.838| Val Acc: 48.000\n",
      "Epoch 288: | Train Loss: 0.64815 | Val Loss: 0.92404 | Train Acc: 59.789| Val Acc: 49.500\n",
      "Epoch 289: | Train Loss: 0.66931 | Val Loss: 0.90357 | Train Acc: 58.902| Val Acc: 50.000\n",
      "Epoch 290: | Train Loss: 0.64657 | Val Loss: 0.93992 | Train Acc: 60.522| Val Acc: 48.125\n",
      "Epoch 291: | Train Loss: 0.67197 | Val Loss: 0.90485 | Train Acc: 57.878| Val Acc: 48.625\n",
      "Epoch 292: | Train Loss: 0.65052 | Val Loss: 0.89669 | Train Acc: 59.804| Val Acc: 50.125\n",
      "Epoch 293: | Train Loss: 0.64938 | Val Loss: 0.91456 | Train Acc: 59.902| Val Acc: 50.000\n",
      "Epoch 294: | Train Loss: 0.65491 | Val Loss: 0.88416 | Train Acc: 59.789| Val Acc: 50.750\n",
      "Epoch 295: | Train Loss: 0.64296 | Val Loss: 0.89782 | Train Acc: 60.533| Val Acc: 50.125\n",
      "Epoch 296: | Train Loss: 0.64656 | Val Loss: 0.91405 | Train Acc: 60.280| Val Acc: 49.250\n",
      "Epoch 297: | Train Loss: 0.64867 | Val Loss: 0.94300 | Train Acc: 59.984| Val Acc: 48.250\n",
      "Epoch 298: | Train Loss: 0.65841 | Val Loss: 0.88673 | Train Acc: 59.120| Val Acc: 50.750\n",
      "Epoch 299: | Train Loss: 0.64579 | Val Loss: 0.88540 | Train Acc: 59.849| Val Acc: 50.375\n",
      "Epoch 300: | Train Loss: 0.65336 | Val Loss: 0.91003 | Train Acc: 59.938| Val Acc: 49.750\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b8862d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_pred_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bc5faca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b8a944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4875, 0.47288755840699054, 0.6907433246633445)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred_list)\n",
    "f1 = f1_score(y_test, y_pred_list, average='macro', labels=np.unique(y_test))\n",
    "roc_auc = multiclass_roc_auc_score(y_test, y_pred_list)\n",
    "acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a18726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
