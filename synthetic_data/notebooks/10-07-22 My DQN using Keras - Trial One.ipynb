{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c47f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#import keras.backend.tensorflow_backend as backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "#from PIL import Image\n",
    "#import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cae091",
   "metadata": {},
   "source": [
    "#### Getting reproducible results I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35533b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "#tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8584e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25892\\3161754494.py:3: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d00651",
   "metadata": {},
   "source": [
    "#### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62071588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>75</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  width  height label\n",
       "0       7      7      49     B\n",
       "1      20     13      75     B\n",
       "2      29      2      81     A\n",
       "3      15      6      39     A\n",
       "4      11      9      71     B"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/dataset_10000.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769ed105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  width  height  label\n",
       "0       7      7      49      1\n",
       "1      20     13      75      1\n",
       "2      29      2      81      0\n",
       "3      15      6      39      0\n",
       "4      11      9      71      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'A':0, 'B':1, 'C':2}\n",
    "df['label'] = df['label'].replace(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1ecf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599486d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59164d8e",
   "metadata": {},
   "source": [
    "#### Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bd289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "#MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "MINIBATCH_SIZE  = 5\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = 'dqn_keras_example'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 20_000\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 50  # episodes\n",
    "SHOW_PREVIEW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780ab08c",
   "metadata": {},
   "source": [
    "#### The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84f047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, X, Y, random=True):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.x = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.y = -1\n",
    "        self.num_classes = 3\n",
    "        self.sample_num = len(X)\n",
    "        self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.total_reward = 0\n",
    "        self.trajectory = []\n",
    "        self.episode_length = 0\n",
    "        self.random = random\n",
    "        self.idx = 0\n",
    "        self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "        \n",
    "    def reset(self, i): #I am going to go through the data sequentially\n",
    "        #print(f'Current epsiode completed. Resetting to index {i}')\n",
    "        if i < self.sample_num:\n",
    "            self.trajectory = []\n",
    "            self.total_reward = 0\n",
    "            self.episode_length = 0\n",
    "            self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "            self.x, self.y = self.X[i], self.Y[i]\n",
    "            self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "            #return self.state, self.available_actions\n",
    "            return self.state\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def get_next_state(self, action):\n",
    "        self.available_actions[0, action] =1\n",
    "        if action < self.num_classes: #the classes\n",
    "            #next_state = None\n",
    "            next_state = copy.deepcopy(self.state)\n",
    "        elif (action >=3) & (action <=5):\n",
    "            feature_idx = action - 3\n",
    "            self.x = self.x.reshape(-1, 3)\n",
    "            x_value = self.x[0, feature_idx]\n",
    "            next_state = copy.deepcopy(self.state)\n",
    "            next_state[0, feature_idx] = x_value\n",
    "        return next_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        ep_length = 1\n",
    "        reward = 0\n",
    "        next_state = self.get_next_state(action)\n",
    "        if action < 3:\n",
    "            if action == self.y:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "            y_actual = self.y \n",
    "            y_pred = action\n",
    "            done = True\n",
    "        else:\n",
    "            reward += 0\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "            done=False\n",
    "            \n",
    "        self.total_reward+=reward\n",
    "        self.episode_length+= ep_length\n",
    "        total_reward_metric = self.total_reward \n",
    "        total_length_metric = self.episode_length\n",
    "        \n",
    "        info = {'episode_length':total_length_metric, 'total_reward': total_reward_metric, 'y_actual':y_actual, \n",
    "                   'y_pred': y_pred}\n",
    "        #print(f'The metrics: {metrics}')\n",
    "        if not self.random:\n",
    "            self.idx +=1\n",
    "        else:\n",
    "            self.idx = random.randint(0, self.sample_num-1)\n",
    "        return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bd267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env(X_train, y_train)\n",
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d824214",
   "metadata": {},
   "source": [
    "#### The DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d056f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self._log_write_dir = self.log_dir\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "        self._train_dir = os.path.join(self._log_write_dir, 'train')\n",
    "        self._train_step = self.model._train_counter\n",
    "\n",
    "        self._val_dir = os.path.join(self._log_write_dir, 'validation')\n",
    "        self._val_step = self.model._test_counter\n",
    "\n",
    "        self._should_write_train_graph = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    def update_stats(self, **stats):\n",
    "        with self.writer.as_default():\n",
    "            for key, value in stats.items():\n",
    "                tf.summary.scalar(key, value, step = self.step)\n",
    "                self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879d4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Main model\n",
    "        self.model = self.create_model2()\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self.create_model2()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=50000)\n",
    "\n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format('keras_dqn_all_feats', int(time.time())))\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 64, input_dim = 3, activation = 'relu'))\n",
    "        model.add(Dense(units = 32, activation='relu'))\n",
    "        model.add(Dense(units = 8, activation = 'relu'))\n",
    "        model.add(Dense(6, activation = 'linear'))\n",
    "        #model.compile(loss='mse', optimizer = Adam(learning_rate=0.001))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def create_model2(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dropout(0.5, input_shape=(1, 3)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(6, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < 1000:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, 64)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            #print(f'current qs list shape: {current_qs_list.shape}')\n",
    "            current_qs = current_qs_list[index]\n",
    "            #print(f'current qs type: {type(current_qs)}')\n",
    "            #print(f'current qs shape: {current_qs.shape}')\n",
    "            #print(f'current qs: {current_qs}')\n",
    "            #print(f'action: {action}')\n",
    "            \n",
    "            try:\n",
    "                current_qs[action] = new_q\n",
    "            except:\n",
    "                current_qs = current_qs.reshape(6,)\n",
    "                current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.asarray(X).astype('float32'), np.asarray(y).astype('float32'), batch_size=64, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state: #if done==True\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > 5:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ebd97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\phd2_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "INFO:tensorflow:Assets written to: models/dqn_keras_example.model\\assets\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\phd2_env\\lib\\site-packages\\ipykernel_launcher.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25892\\307148213.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#print(f'TRAINING AGENT')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25892\\640385301.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, terminal_state, step)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# Fit on all samples as one batch, log only on terminal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mterminal_state\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Update target network counter every episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "\n",
    "#for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "for episode in range(1, 5000):\n",
    "    if episode%1000 == 0:\n",
    "        print(f'Episode: {episode}')\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    #i = random.randint(0, env.sample_num-1)\n",
    "    current_state = env.reset(env.idx)\n",
    "    #print(f'X: {env.x}')\n",
    "    #print(f'y: {env.y}')\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "        #print(f'Step: {step}')\n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "            #current_state = current_state.reshape((3,))\n",
    "            q_values = agent.get_qs(current_state)\n",
    "            #q_values = self.dqn.predict_q(state) #get q values for all actions shape(1, 31)\n",
    "            #print(f'q values: {q_values}')\n",
    "            available_q_values = q_values[env.available_actions==0] #q-values for actions not yet selected e.g. shape(20,)\n",
    "            action_q = np.max(available_q_values)\n",
    "            #action_q = np.max(q_values)\n",
    "            action = np.where(q_values[0]==action_q)[0][0] #index of action with max q-value\n",
    "        else:\n",
    "            #print('Choosing randomly')\n",
    "            available_indices = np.where(env.available_actions==0)[1]\n",
    "            action = random.choice(available_indices)  \n",
    "            #action_index = random.randrange(self.n_actions) \n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        #if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "        #    env.render()\n",
    "        \n",
    "        #print(f'current state: {current_state}')\n",
    "        #print(f'action: {action}')\n",
    "        #print(f'reward: {reward}')\n",
    "        #print(f'new state: {new_state}')\n",
    "        #print(f'done: {done}')\n",
    "        \n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        #print(f'TRAINING AGENT')\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if min_reward >= MIN_REWARD:\n",
    "            #agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "            agent.model.save(f'models/{MODEL_NAME}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500160f",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94df3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "\n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "    #print(f'Roc auc dict: {roc_auc_dict}')\n",
    "    avg = sum(roc_auc_dict.values()) / len(roc_auc_dict)\n",
    "    \n",
    "    #return roc_auc_dict\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c97b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ytest, ypred):\n",
    "    acc = accuracy_score(ytest, ypred)\n",
    "    f1 = f1_score(ytest, ypred, average ='macro', labels=np.unique(ytest))\n",
    "    try:\n",
    "        roc_auc = multiclass(ytest, ypred)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    return acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_env = Env(X_test, y_test,random=False)\n",
    "test_df = pd.DataFrame()\n",
    "testing_env.reset(testing_env.idx)\n",
    "for episode in tqdm(range(1, len(X_test)+1), ascii=True, unit='episodes'):\n",
    "    #print(f'episode: {episode}, index: {testing_env.idx}')\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = testing_env.reset(testing_env.idx)\n",
    "    #print(f'current state: {current_state}')\n",
    "    #print(f'current y: {testing_env.y}')\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        action = np.argmax(agent.get_qs(current_state))\n",
    "        #print(f'action using agent: {action}')\n",
    "        new_state, reward, done, info = testing_env.step(action)\n",
    "        #print(f'new state: {new_state}')\n",
    "        #print(f'reward: {reward}')\n",
    "        #print(f'done: {done}')\n",
    "        #print(f'info: {info}')\n",
    "        if done == True:\n",
    "            test_df = test_df.append(info, ignore_index=True)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        #if SHOW_PREVIEW and not episode % 1000:\n",
    "        #    testing_env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        #agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        #agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "#     ep_rewards.append(episode_reward)\n",
    "#     if not episode % 1000 or episode == 1:\n",
    "#         average_reward = sum(ep_rewards[-1000:])/len(ep_rewards[-1000:])\n",
    "#         min_reward = min(ep_rewards[-1000:])\n",
    "#         max_reward = max(ep_rewards[-1000:])\n",
    "#         agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "#         # Save model, but only when min reward is greater or equal a set value\n",
    "#         #if min_reward >= MIN_REWARD:\n",
    "#         if min_reward >= -200:\n",
    "#             #agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "#             agent.model.save(f'models/keras_dqn_all_feats.model')\n",
    "#     # Decay epsilon\n",
    "#     if epsilon > 0.001:\n",
    "#         epsilon *= 0.99975\n",
    "#         epsilon = max(0.001, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa9a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
