{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505079a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cbf581",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tensorflow.set_random_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17be21",
   "metadata": {},
   "source": [
    "#### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b798c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset_10000.csv')\n",
    "class_dict = {'A':0, 'B':1, 'C':2}\n",
    "df['label'] = df['label'].replace(class_dict)\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "987f99be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43766d06",
   "metadata": {},
   "source": [
    "#### The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f2a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd69676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticEnv(Env):\n",
    "    def __init__(self, X, Y, random=True):\n",
    "        super(SyntheticEnv, self).__init__()\n",
    "        self.action_space = Discrete(6)\n",
    "        self.observation_space = Box(0, 1.5, (3,))\n",
    "        self.actions = ['A', 'B', 'C', 'length', 'width', 'height']\n",
    "        self.max_steps = 7\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sample_num = len(X)\n",
    "        self.idx = -1\n",
    "        self.x = np.zeros((3,), dtype=np.float32)\n",
    "        self.y = np.nan\n",
    "        self.state = np.zeros((3,), dtype=np.float32)\n",
    "        self.num_classes = 3\n",
    "        self.episode_length = 0\n",
    "        self.trajectory = []\n",
    "        self.total_reward = 0\n",
    "        self.random = random\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        #print('A step in the environment')\n",
    "        #print(f'action: {action}')\n",
    "        self.episode_length += 1\n",
    "        reward = 0\n",
    "        if self.episode_length == self.max_steps: # episode too long\n",
    "            #print('Reached max steps')\n",
    "            reward -=1\n",
    "            self.total_reward -=1\n",
    "            terminated = True\n",
    "            done = True\n",
    "            y_actual = self.y\n",
    "            y_pred = np.nan\n",
    "        elif action < self.num_classes: #diagnosis (terminal action)\n",
    "            #print('Terminal action')\n",
    "            if action == self.y:\n",
    "                reward +=1\n",
    "                self.total_reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "                self.total_reward -= 1\n",
    "            terminated = False\n",
    "            done = True\n",
    "            y_actual = self.y\n",
    "            y_pred = action\n",
    "        elif self.actions[action] in self.trajectory: #action already picked \n",
    "            #print('Repeated action')\n",
    "            terminated = False\n",
    "            reward -= 1\n",
    "            self.total_reward -= 1\n",
    "            done = False\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "        else: #new feature being acquired\n",
    "            #print('Acquiring new feature')\n",
    "            terminated = False\n",
    "            reward += 1\n",
    "            self.total_reward += 1\n",
    "            done = False\n",
    "            self.state = self.get_next_state(action-self.num_classes)\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "        self.trajectory.append(self.actions[action])\n",
    "        info = {'index': self.idx, 'episode_length':self.episode_length, 'reward': self.total_reward, 'y_pred': y_pred, \n",
    "                'y_actual': y_actual, 'trajectory':self.trajectory, 'terminated':terminated}\n",
    "        #self.render()\n",
    "        return self.state, reward, done, info\n",
    "            \n",
    "    \n",
    "    def render(self):\n",
    "        print(f'STEP {self.episode_length} for index {self.idx}')\n",
    "        print(f'x: {self.x}')\n",
    "        print(f'y: {self.y}')\n",
    "        print(f'Current state: {self.state}')\n",
    "        print(f'Total reward: {self.total_reward}')\n",
    "        print(f'Trajectory: {self.trajectory}')\n",
    "        \n",
    "            \n",
    "    \n",
    "    def reset(self):\n",
    "        #print('RESETTING THE ENVIRONMENT')\n",
    "        if self.random:\n",
    "            self.idx = random.randint(0, self.sample_num-1)\n",
    "        else:\n",
    "            self.idx += 1\n",
    "            if self.idx == len(self.X):\n",
    "                raise StopIteration()\n",
    "        #print(f'New idx: {self.idx}')\n",
    "        self.x, self.y = self.X[self.idx], self.Y[self.idx]\n",
    "        #print(f'New x: {self.x}')\n",
    "        #print(f'New y: {self.y}')\n",
    "        self.state = np.zeros((3,), dtype=np.float32)\n",
    "        #print(f'New state: {self.state}')\n",
    "        self.trajectory = []\n",
    "        #print(f'New trajectory: {self.trajectory}')\n",
    "        self.episode_length = 0\n",
    "        #print(f'New episode length: {self.episode_length}')\n",
    "        self.total_reward = 0\n",
    "        #print(f'New total reward: {self.total_reward}')\n",
    "        return self.state\n",
    "        \n",
    "    \n",
    "    def get_next_state(self, feature_idx):\n",
    "        self.x = self.x.reshape(-1, 3)\n",
    "        x_value = self.x[0, feature_idx]\n",
    "        next_state = copy.deepcopy(self.state)\n",
    "        next_state[feature_idx] = x_value\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "500968c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines.common.env_checker import check_env\n",
    "# from stable_baselines.common.policies import MlpPolicy\n",
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines import PPO2\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "from baselines.ppo2 import ppo2\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from baselines import bench\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.common.tf_util import make_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088016c",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f482fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/synthetic_deepq_dqn\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\baselines\\common\\input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\baselines\\common\\models.py:94: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 207      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 0.2      |\n",
      "| steps                   | 426      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 0.2      |\n",
      "| steps                   | 623      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 788      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 0.2      |\n",
      "| steps                   | 999      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 1.22e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 87       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 1.46e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 1.68e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 1.89e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1e+03    |\n",
      "| mean 100 episode reward | -0       |\n",
      "| steps                   | 2.1e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 1.1e+03  |\n",
      "| mean 100 episode reward | 0.1      |\n",
      "| steps                   | 2.30e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 1.2e+03  |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 2.49e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 78       |\n",
      "| episodes                | 1.3e+03  |\n",
      "| mean 100 episode reward | 0.2      |\n",
      "| steps                   | 2.66e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 1.4e+03  |\n",
      "| mean 100 episode reward | 0.1      |\n",
      "| steps                   | 2.87e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 74       |\n",
      "| episodes                | 1.5e+03  |\n",
      "| mean 100 episode reward | 0        |\n",
      "| steps                   | 3.05e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 1.6e+03  |\n",
      "| mean 100 episode reward | 0.1      |\n",
      "| steps                   | 3.26e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 71       |\n",
      "| episodes                | 1.7e+03  |\n",
      "| mean 100 episode reward | 0.1      |\n",
      "| steps                   | 3.48e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 69       |\n",
      "| episodes                | 1.8e+03  |\n",
      "| mean 100 episode reward | -0.1     |\n",
      "| steps                   | 3.69e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 68       |\n",
      "| episodes                | 1.9e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 3.87e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 2e+03    |\n",
      "| mean 100 episode reward | -0       |\n",
      "| steps                   | 4.08e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 2.1e+03  |\n",
      "| mean 100 episode reward | -0       |\n",
      "| steps                   | 4.26e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 2.2e+03  |\n",
      "| mean 100 episode reward | -0       |\n",
      "| steps                   | 4.45e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 2.3e+03  |\n",
      "| mean 100 episode reward | -0.3     |\n",
      "| steps                   | 4.62e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 60       |\n",
      "| episodes                | 2.4e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 4.82e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 2.5e+03  |\n",
      "| mean 100 episode reward | -0.1     |\n",
      "| steps                   | 5.02e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 56       |\n",
      "| episodes                | 2.6e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 5.23e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 2.7e+03  |\n",
      "| mean 100 episode reward | -0.3     |\n",
      "| steps                   | 5.43e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 53       |\n",
      "| episodes                | 2.8e+03  |\n",
      "| mean 100 episode reward | -0.1     |\n",
      "| steps                   | 5.65e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 2.9e+03  |\n",
      "| mean 100 episode reward | -0.1     |\n",
      "| steps                   | 5.87e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 50       |\n",
      "| episodes                | 3e+03    |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 6.05e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 48       |\n",
      "| episodes                | 3.1e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 6.22e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 3.2e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 6.4e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 3.3e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 6.61e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 44       |\n",
      "| episodes                | 3.4e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 6.78e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 42       |\n",
      "| episodes                | 3.5e+03  |\n",
      "| mean 100 episode reward | -0.3     |\n",
      "| steps                   | 6.95e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 41       |\n",
      "| episodes                | 3.6e+03  |\n",
      "| mean 100 episode reward | -0.5     |\n",
      "| steps                   | 7.15e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 39       |\n",
      "| episodes                | 3.7e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 7.34e+03 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 38       |\n",
      "| episodes                | 3.8e+03  |\n",
      "| mean 100 episode reward | -0.5     |\n",
      "| steps                   | 7.5e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 36       |\n",
      "| episodes                | 3.9e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 7.67e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 4e+03    |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 7.81e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 4.1e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 7.98e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 32       |\n",
      "| episodes                | 4.2e+03  |\n",
      "| mean 100 episode reward | -0.3     |\n",
      "| steps                   | 8.13e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 4.3e+03  |\n",
      "| mean 100 episode reward | -0.6     |\n",
      "| steps                   | 8.3e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 30       |\n",
      "| episodes                | 4.4e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 8.47e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 28       |\n",
      "| episodes                | 4.5e+03  |\n",
      "| mean 100 episode reward | -0.9     |\n",
      "| steps                   | 8.64e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 4.6e+03  |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 8.77e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 26       |\n",
      "| episodes                | 4.7e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 8.96e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 24       |\n",
      "| episodes                | 4.8e+03  |\n",
      "| mean 100 episode reward | -0.5     |\n",
      "| steps                   | 9.14e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 23       |\n",
      "| episodes                | 4.9e+03  |\n",
      "| mean 100 episode reward | -0.6     |\n",
      "| steps                   | 9.3e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 22       |\n",
      "| episodes                | 5e+03    |\n",
      "| mean 100 episode reward | -0.2     |\n",
      "| steps                   | 9.44e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 20       |\n",
      "| episodes                | 5.1e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 9.61e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 19       |\n",
      "| episodes                | 5.2e+03  |\n",
      "| mean 100 episode reward | -0.4     |\n",
      "| steps                   | 9.75e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 18       |\n",
      "| episodes                | 5.3e+03  |\n",
      "| mean 100 episode reward | -0.5     |\n",
      "| steps                   | 9.91e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 15       |\n",
      "| episodes                | 5.4e+03  |\n",
      "| mean 100 episode reward | -0.8     |\n",
      "| steps                   | 1.02e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 11       |\n",
      "| episodes                | 5.5e+03  |\n",
      "| mean 100 episode reward | -2       |\n",
      "| steps                   | 1.07e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 5.6e+03  |\n",
      "| mean 100 episode reward | -1.3     |\n",
      "| steps                   | 1.13e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 5.7e+03  |\n",
      "| mean 100 episode reward | -2.2     |\n",
      "| steps                   | 1.19e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 5.8e+03  |\n",
      "| mean 100 episode reward | -0.8     |\n",
      "| steps                   | 1.24e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 5.9e+03  |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 1.28e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6e+03    |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 1.32e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.1e+03  |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 1.36e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.2e+03  |\n",
      "| mean 100 episode reward | 0.6      |\n",
      "| steps                   | 1.4e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.3e+03  |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 1.45e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.4e+03  |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 1.5e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.5e+03  |\n",
      "| mean 100 episode reward | 0.3      |\n",
      "| steps                   | 1.55e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.6e+03  |\n",
      "| mean 100 episode reward | 0.4      |\n",
      "| steps                   | 1.6e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.7e+03  |\n",
      "| mean 100 episode reward | 0.5      |\n",
      "| steps                   | 1.66e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.8e+03  |\n",
      "| mean 100 episode reward | 1.2      |\n",
      "| steps                   | 1.71e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 6.9e+03  |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 1.76e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7e+03    |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 1.81e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.1e+03  |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 1.86e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.2e+03  |\n",
      "| mean 100 episode reward | 1.3      |\n",
      "| steps                   | 1.91e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.3e+03  |\n",
      "| mean 100 episode reward | 1.4      |\n",
      "| steps                   | 1.95e+04 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model due to mean reward increase: None -> 1.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.4e+03  |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 2e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.5e+03  |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 2.05e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.6e+03  |\n",
      "| mean 100 episode reward | 1.6      |\n",
      "| steps                   | 2.1e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.7e+03  |\n",
      "| mean 100 episode reward | 1.3      |\n",
      "| steps                   | 2.15e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.8e+03  |\n",
      "| mean 100 episode reward | 1.2      |\n",
      "| steps                   | 2.2e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 7.9e+03  |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 2.25e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8e+03    |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 2.3e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.1e+03  |\n",
      "| mean 100 episode reward | 1.2      |\n",
      "| steps                   | 2.34e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.2e+03  |\n",
      "| mean 100 episode reward | 1.2      |\n",
      "| steps                   | 2.39e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.3e+03  |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 2.44e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.4e+03  |\n",
      "| mean 100 episode reward | 1.2      |\n",
      "| steps                   | 2.49e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.5e+03  |\n",
      "| mean 100 episode reward | 1.4      |\n",
      "| steps                   | 2.54e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.6e+03  |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 2.59e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.7e+03  |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 2.63e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.8e+03  |\n",
      "| mean 100 episode reward | 1.3      |\n",
      "| steps                   | 2.68e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 8.9e+03  |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 2.73e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9e+03    |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 2.78e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.1e+03  |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 2.82e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.2e+03  |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 2.87e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.3e+03  |\n",
      "| mean 100 episode reward | 1.8      |\n",
      "| steps                   | 2.91e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.4e+03  |\n",
      "| mean 100 episode reward | 1.8      |\n",
      "| steps                   | 2.96e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 1.0 -> 1.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.5e+03  |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 3e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.6e+03  |\n",
      "| mean 100 episode reward | 2        |\n",
      "| steps                   | 3.05e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.7e+03  |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 3.09e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.8e+03  |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 3.14e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 9.9e+03  |\n",
      "| mean 100 episode reward | 1.6      |\n",
      "| steps                   | 3.19e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1e+04    |\n",
      "| mean 100 episode reward | 1.5      |\n",
      "| steps                   | 3.24e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.01e+04 |\n",
      "| mean 100 episode reward | 2        |\n",
      "| steps                   | 3.29e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.02e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 3.33e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.03e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 3.37e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.04e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 3.42e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.05e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 3.46e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.06e+04 |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 3.51e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.07e+04 |\n",
      "| mean 100 episode reward | 2        |\n",
      "| steps                   | 3.56e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.08e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 3.6e+04  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.09e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 3.65e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.1e+04  |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 3.69e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.11e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 3.74e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.12e+04 |\n",
      "| mean 100 episode reward | 1.8      |\n",
      "| steps                   | 3.79e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.13e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 3.83e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.14e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 3.88e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.15e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 3.92e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.16e+04 |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 3.97e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 1.8 -> 2.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.17e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 4.01e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.18e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 4.06e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.19e+04 |\n",
      "| mean 100 episode reward | 1.9      |\n",
      "| steps                   | 4.11e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.2e+04  |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 4.15e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.21e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 4.19e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.22e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.24e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.23e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 4.28e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.24e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 4.32e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.25e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 4.37e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.26e+04 |\n",
      "| mean 100 episode reward | 1.8      |\n",
      "| steps                   | 4.41e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.27e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 4.46e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.28e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 4.5e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.29e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.55e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.3e+04  |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 4.59e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.31e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.64e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.32e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 4.68e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.33e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.73e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.34e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.77e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.35e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.81e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.36e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.86e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.37e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 4.91e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.38e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 4.95e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.39e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 5e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.4e+04  |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 5.04e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.41e+04 |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 5.08e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.42e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 5.13e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.43e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 5.17e+04 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.44e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 5.22e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.45e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 5.26e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.46e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 5.3e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.47e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 5.35e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.48e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 5.39e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.49e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 5.43e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.5e+04  |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 5.48e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.51e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 5.53e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.52e+04 |\n",
      "| mean 100 episode reward | 2.2      |\n",
      "| steps                   | 5.57e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.53e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 5.62e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.54e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 5.66e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.55e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 5.7e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.56e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 5.75e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.57e+04 |\n",
      "| mean 100 episode reward | 2.4      |\n",
      "| steps                   | 5.79e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.58e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 5.84e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.59e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 5.88e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.6e+04  |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 5.92e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.61e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 5.96e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2.3 -> 3.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.62e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 6.01e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.63e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.05e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.64e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.09e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.65e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 6.14e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.66e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 6.18e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.67e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 6.22e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.68e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 6.27e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.69e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.31e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.7e+04  |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.35e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.71e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 6.4e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.72e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 6.44e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.73e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.48e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.74e+04 |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 6.52e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.75e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 6.57e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.76e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 6.61e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.77e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 6.65e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.78e+04 |\n",
      "| mean 100 episode reward | 2.5      |\n",
      "| steps                   | 6.7e+04  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.79e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 6.74e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.8e+04  |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 6.78e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.81e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 6.83e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.82e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 6.87e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.83e+04 |\n",
      "| mean 100 episode reward | 2.3      |\n",
      "| steps                   | 6.92e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.84e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 6.96e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 3.0 -> 3.4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.85e+04 |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 7e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.86e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 7.05e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.87e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 7.09e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.88e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 7.13e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.89e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 7.18e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.9e+04  |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 7.22e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.91e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 7.26e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.92e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 7.31e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.93e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 7.35e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.94e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 7.39e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.95e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 7.44e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.96e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 7.48e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.97e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 7.52e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.98e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 7.57e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.99e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 7.61e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2e+04    |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 7.65e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.01e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 7.69e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.02e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 7.74e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.03e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 7.78e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.04e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 7.82e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.05e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 7.87e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.06e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 7.91e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.07e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 7.95e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.08e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 8e+04    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.09e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 8.04e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.1e+04  |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.08e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.11e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.13e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.12e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 8.17e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.13e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.22e+04 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.14e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 8.26e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.15e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 8.3e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.16e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 8.35e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.17e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 8.39e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.18e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.43e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.19e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.47e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.2e+04  |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 8.51e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.21e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.56e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.22e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 8.6e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.23e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 8.64e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.24e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.69e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.25e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 8.73e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.26e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.77e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.27e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 8.81e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.28e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 8.86e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.29e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 8.9e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.3e+04  |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 8.94e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.31e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 8.98e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.32e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 9.03e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.33e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.07e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.34e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.11e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.35e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 9.16e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.36e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 9.2e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.37e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 9.24e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.38e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.29e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.39e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 9.33e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.4e+04  |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 9.37e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.41e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 9.42e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.42e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 9.46e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.43e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 9.5e+04  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.44e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 9.55e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.45e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 9.59e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.46e+04 |\n",
      "| mean 100 episode reward | 2.6      |\n",
      "| steps                   | 9.63e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.47e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 9.68e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.48e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.72e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.49e+04 |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 9.76e+04 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.5e+04  |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.81e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.51e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.85e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.52e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 9.89e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.53e+04 |\n",
      "| mean 100 episode reward | 2.7      |\n",
      "| steps                   | 9.94e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.54e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 9.98e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.55e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1e+05    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.56e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.01e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.57e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.01e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.58e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.02e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.59e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 1.02e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.6e+04  |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.02e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.61e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.03e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.62e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 1.03e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.63e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.04e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.64e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.04e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.65e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.05e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.66e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.05e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.67e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 1.05e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.68e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.06e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.69e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.06e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.7e+04  |\n",
      "| mean 100 episode reward | 3.5      |\n",
      "| steps                   | 1.07e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.71e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.07e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.72e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.08e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.73e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.08e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.74e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 1.08e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.75e+04 |\n",
      "| mean 100 episode reward | 3.3      |\n",
      "| steps                   | 1.09e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.76e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 1.09e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.77e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 1.1e+05  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.78e+04 |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 1.1e+05  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.79e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.11e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.8e+04  |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 1.11e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.81e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.11e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.82e+04 |\n",
      "| mean 100 episode reward | 2.9      |\n",
      "| steps                   | 1.12e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.83e+04 |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 1.12e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.84e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 1.13e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.85e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.13e+05 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.86e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.14e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.87e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.14e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.88e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.14e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.89e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.15e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.9e+04  |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.15e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.91e+04 |\n",
      "| mean 100 episode reward | 2.8      |\n",
      "| steps                   | 1.16e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.92e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.16e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.93e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.17e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.94e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.17e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.95e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.18e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.96e+04 |\n",
      "| mean 100 episode reward | 3.1      |\n",
      "| steps                   | 1.18e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.97e+04 |\n",
      "| mean 100 episode reward | 3        |\n",
      "| steps                   | 1.18e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.98e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.19e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.99e+04 |\n",
      "| mean 100 episode reward | 3.2      |\n",
      "| steps                   | 1.19e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 3e+04    |\n",
      "| mean 100 episode reward | 3.4      |\n",
      "| steps                   | 1.2e+05  |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: 3.4\n",
      "DQN Training Time: 87.67430996894836\n"
     ]
    }
   ],
   "source": [
    "def deepq_dqn():\n",
    "    logger.configure(dir='./logs/synthetic_deepq_dqn', format_strs=['stdout', 'tensorboard'])\n",
    "    env = SyntheticEnv(X_train, y_train)\n",
    "    env = bench.Monitor(env, logger.get_dir())\n",
    "\n",
    "    model = deepq.learn(\n",
    "        env,\n",
    "        'mlp',\n",
    "        num_layers=1,\n",
    "        num_hidden=64,\n",
    "        activation=tf.nn.relu,\n",
    "        hiddens=[32],\n",
    "        dueling=False,\n",
    "        lr=1e-4,\n",
    "        total_timesteps=int(1.2e5),\n",
    "        buffer_size=10000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.01,\n",
    "        train_freq=4,\n",
    "        learning_starts=10000,\n",
    "        target_network_update_freq=1000,\n",
    "    )\n",
    "\n",
    "    model.save('models/synthetic_deepq_dqn.pkl')\n",
    "    env.close()\n",
    "\n",
    "    return model\n",
    "\n",
    "start_time = time.time()\n",
    "dqn_model = deepq_dqn()\n",
    "print(\"DQN Training Time:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a37132",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e1b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd172c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass(actual_class, pred_class, average = 'macro'):\n",
    "\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "    avg = sum(roc_auc_dict.values()) / len(roc_auc_dict)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c9fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ytest, ypred):\n",
    "    acc = accuracy_score(ytest, ypred)\n",
    "    f1 = f1_score(ytest, ypred, average ='macro', labels=np.unique(ytest))\n",
    "    try:\n",
    "        roc_auc = multiclass(ytest, ypred)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    return acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_length_reward(df):\n",
    "    length = np.mean(df.episode_length)\n",
    "    reward = np.mean(df.reward)\n",
    "    return length, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c632b866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing done.....\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dqn_eval(dqn_model):\n",
    "    attempts, correct = 0,0\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    env = SyntheticEnv(X_test, y_test, random=False)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done,info = env.step(dqn_model(obs[None])[0])\n",
    "                #if (done==True) & (np.isfinite(info['y_pred'])):\n",
    "                if done == True:\n",
    "                    test_df = test_df.append(info, ignore_index=True)\n",
    "                #print('....................TEST DF ....................')\n",
    "                #if len(test_df) != 0:\n",
    "                #    print(test_df.head())\n",
    "\n",
    "    except StopIteration:\n",
    "        print('Testing done.....')\n",
    "    return test_df\n",
    "\n",
    "test_df = synthetic_dqn_eval(dqn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa22e3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7997e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2195"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df = test_df[test_df['y_pred'].notna()]\n",
    "success_df = y_pred_df[y_pred_df['y_pred']== y_pred_df['y_actual']]\n",
    "len(success_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b03bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.16666666666667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success_rate = len(success_df)/len(test_df)*100\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc37523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.3806666666666665, 2.8213333333333335)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eavg length and return \n",
    "avg_length, avg_return = get_avg_length_reward(test_df)\n",
    "avg_length, avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4367594b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8468364197530864, 0.8438350607706327, 0.8745244159312989)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1, roc_auc = test(y_pred_df['y_actual'], y_pred_df['y_pred'])\n",
    "acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760d2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a23e7",
   "metadata": {},
   "source": [
    "#### Analysis for episodes that exceed maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "179aba57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>index</th>\n",
       "      <th>reward</th>\n",
       "      <th>terminated</th>\n",
       "      <th>trajectory</th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_length  index  reward  terminated  \\\n",
       "1              7.0    1.0    -1.0         1.0   \n",
       "12             7.0   12.0    -1.0         1.0   \n",
       "18             7.0   18.0    -1.0         1.0   \n",
       "19             7.0   19.0    -1.0         1.0   \n",
       "20             7.0   20.0    -1.0         1.0   \n",
       "\n",
       "                                           trajectory  y_actual  y_pred  \n",
       "1   [height, length, width, width, width, width, w...       1.0     NaN  \n",
       "12  [height, length, width, width, width, width, w...       0.0     NaN  \n",
       "18  [height, length, width, width, width, width, w...       1.0     NaN  \n",
       "19  [height, length, width, width, width, width, w...       1.0     NaN  \n",
       "20  [height, length, width, width, width, width, w...       0.0     NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_df = test_df[test_df['y_pred'].isna()]\n",
    "max_length_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e134ce80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max_length_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d0db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>index</th>\n",
       "      <th>reward</th>\n",
       "      <th>terminated</th>\n",
       "      <th>trajectory</th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2948.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, width, width, width, width, w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, length, length, length, lengt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2968.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, length, length, length, lengt...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2978.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, length, length, length, lengt...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[height, length, length, length, length, lengt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_length   index  reward  terminated  \\\n",
       "2948             7.0  2948.0    -1.0         1.0   \n",
       "2961             7.0  2961.0    -3.0         1.0   \n",
       "2968             7.0  2968.0    -3.0         1.0   \n",
       "2978             7.0  2978.0    -3.0         1.0   \n",
       "2986             7.0  2986.0    -3.0         1.0   \n",
       "\n",
       "                                             trajectory  y_actual  y_pred  \n",
       "2948  [height, length, width, width, width, width, w...       0.0     NaN  \n",
       "2961  [height, length, length, length, length, lengt...       1.0     NaN  \n",
       "2968  [height, length, length, length, length, lengt...       2.0     NaN  \n",
       "2978  [height, length, length, length, length, lengt...       2.0     NaN  \n",
       "2986  [height, length, length, length, length, lengt...       1.0     NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cba99ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at misdiagnosed episodes\n",
    "\n",
    "# Look at episodes that exceed max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53fe37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
