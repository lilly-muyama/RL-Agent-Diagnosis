{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1677aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484dae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tensorflow.set_random_seed(SEED)\n",
    "#tensorflow.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d5dc7",
   "metadata": {},
   "source": [
    "#### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3296e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset_10000.csv')\n",
    "class_dict = {'A':0, 'B':1, 'C':2}\n",
    "df['label'] = df['label'].replace(class_dict)\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714feb3d",
   "metadata": {},
   "source": [
    "#### The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f836162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4c9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticEnv(Env):\n",
    "    def __init__(self, X, Y, random=True):\n",
    "        super(SyntheticEnv, self).__init__()\n",
    "        self.action_space = Discrete(6)\n",
    "        self.observation_space = Box(0, 1.5, (3,))\n",
    "        self.actions = ['A', 'B', 'C', 'length', 'width', 'height']\n",
    "        self.max_steps = 7\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sample_num = len(X)\n",
    "        self.idx = -1\n",
    "        self.x = np.zeros((3,), dtype=np.float32)\n",
    "        self.y = np.nan\n",
    "        self.state = np.zeros((3,), dtype=np.float32)\n",
    "        self.num_classes = 3\n",
    "        self.episode_length = 0\n",
    "        self.trajectory = []\n",
    "        self.total_reward = 0\n",
    "        self.random = random\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        #print('A step in the environment')\n",
    "        #print(f'action: {action}')\n",
    "        self.episode_length += 1\n",
    "        reward = 0\n",
    "        if self.episode_length == self.max_steps: # episode too long\n",
    "            #print('Reached max steps')\n",
    "            reward -=1\n",
    "            self.total_reward -=1\n",
    "            terminated = True\n",
    "            done = True\n",
    "            y_actual = self.y\n",
    "            y_pred = np.nan\n",
    "        elif action < self.num_classes: #diagnosis (terminal action)\n",
    "            #print('Terminal action')\n",
    "            if action == self.y:\n",
    "                reward +=1\n",
    "                self.total_reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "                self.total_reward -= 1\n",
    "            terminated = False\n",
    "            done = True\n",
    "            y_actual = self.y\n",
    "            y_pred = action\n",
    "        elif self.actions[action] in self.trajectory: #action already picked \n",
    "            #print('Repeated action')\n",
    "            terminated = False\n",
    "            reward -= 1\n",
    "            self.total_reward -= 1\n",
    "            done = False\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "        else: #new feature being acquired\n",
    "            #print('Acquiring new feature')\n",
    "            terminated = False\n",
    "            reward += 1\n",
    "            self.total_reward += 1\n",
    "            done = False\n",
    "            self.state = self.get_next_state(action-self.num_classes)\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "        self.trajectory.append(self.actions[action])\n",
    "        info = {'index': self.idx, 'episode_length':self.episode_length, 'reward': self.total_reward, 'y_pred': y_pred, \n",
    "                'y_actual': y_actual, 'trajectory':self.trajectory, 'terminated':terminated}\n",
    "        #self.render()\n",
    "        return self.state, reward, done, info\n",
    "            \n",
    "    \n",
    "    def render(self):\n",
    "        print(f'STEP {self.episode_length} for index {self.idx}')\n",
    "        print(f'x: {self.x}')\n",
    "        print(f'y: {self.y}')\n",
    "        print(f'Current state: {self.state}')\n",
    "        print(f'Total reward: {self.total_reward}')\n",
    "        print(f'Trajectory: {self.trajectory}')\n",
    "        \n",
    "            \n",
    "    \n",
    "    def reset(self):\n",
    "        #print('RESETTING THE ENVIRONMENT')\n",
    "        if self.random:\n",
    "            self.idx = random.randint(0, self.sample_num-1)\n",
    "        else:\n",
    "            self.idx += 1\n",
    "            if self.idx == len(self.X):\n",
    "                raise StopIteration()\n",
    "        #print(f'New idx: {self.idx}')\n",
    "        self.x, self.y = self.X[self.idx], self.Y[self.idx]\n",
    "        #print(f'New x: {self.x}')\n",
    "        #print(f'New y: {self.y}')\n",
    "        self.state = np.zeros((3,), dtype=np.float32)\n",
    "        #print(f'New state: {self.state}')\n",
    "        self.trajectory = []\n",
    "        #print(f'New trajectory: {self.trajectory}')\n",
    "        self.episode_length = 0\n",
    "        #print(f'New episode length: {self.episode_length}')\n",
    "        self.total_reward = 0\n",
    "        #print(f'New total reward: {self.total_reward}')\n",
    "        return self.state\n",
    "        \n",
    "    \n",
    "    def get_next_state(self, feature_idx):\n",
    "        self.x = self.x.reshape(-1, 3)\n",
    "        x_value = self.x[0, feature_idx]\n",
    "        next_state = copy.deepcopy(self.state)\n",
    "        next_state[feature_idx] = x_value\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fe115",
   "metadata": {},
   "source": [
    "#### The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdf434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "#from stable_baselines import DQN\n",
    "from stable_baselines import deepq\n",
    "import tensorflow as tf\n",
    "from stable_baselines import bench, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdef4e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'MlpPolicy' has no attribute '_init_num_timesteps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28516\\3554013347.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdqn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstable_dqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28516\\3554013347.py\u001b[0m in \u001b[0;36mstable_dqn\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSyntheticEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbench\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/synthetic_stable_dueing.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_v1_env\\lib\\site-packages\\stable_baselines\\deepq\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[0;32m    153\u001b[0m               reset_num_timesteps=True, replay_wrapper=None):\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mnew_tb_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_num_timesteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'MlpPolicy' has no attribute '_init_num_timesteps'"
     ]
    }
   ],
   "source": [
    "def stable_dqn():\n",
    "    env = SyntheticEnv(X_train, y_train)\n",
    "    env = bench.Monitor(env, logger.get_dir())\n",
    "    model = deepq.DQN.learn(MlpPolicy, env)\n",
    "    model.save()\n",
    "    env.close()\n",
    "    return model\n",
    "def stable_dqn():\n",
    "    env = SyntheticEnv(X_train, y_train)\n",
    "    env = bench.Monitor(env, logger.get_dir())\n",
    "    model = DQN('MlpPolicy', training_env, verbose=1)\n",
    "    model.learn(total_timesteps=int(1.2e5), log_interval=10000)\n",
    "    model.save('models/synthetic_stable_double.pkl')\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "dqn_model = stable_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24036b72",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e519d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass(actual_class, pred_class, average = 'macro'):\n",
    "\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "    avg = sum(roc_auc_dict.values()) / len(roc_auc_dict)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ytest, ypred):\n",
    "    acc = accuracy_score(ytest, ypred)\n",
    "    f1 = f1_score(ytest, ypred, average ='macro', labels=np.unique(ytest))\n",
    "    try:\n",
    "        roc_auc = multiclass(ytest, ypred)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    return acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_length_reward(df):\n",
    "    length = np.mean(df.episode_length)\n",
    "    reward = np.mean(df.reward)\n",
    "    return length, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_dqn_eval(dqn_model):\n",
    "    attempts, correct = 0,0\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "    env = SyntheticEnv(X_test, y_test, random=False)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done,info = env.step(dqn_model(obs[None])[0])\n",
    "                #if (done==True) & (np.isfinite(info['y_pred'])):\n",
    "                if done == True:\n",
    "                    test_df = test_df.append(info, ignore_index=True)\n",
    "                #print('....................TEST DF ....................')\n",
    "                #if len(test_df) != 0:\n",
    "                #    print(test_df.head())\n",
    "\n",
    "    except StopIteration:\n",
    "        print('Testing done.....')\n",
    "    return test_df\n",
    "\n",
    "test_df = synthetic_dqn_eval(dqn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48839273",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = test_df[test_df['y_pred'].notna()]\n",
    "success_df = y_pred_df[y_pred_df['y_pred']== y_pred_df['y_actual']]\n",
    "len(success_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591f4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = len(success_df)/len(test_df)*100\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1556be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg length and return \n",
    "avg_length, avg_return = get_avg_length_reward(test_df)\n",
    "avg_length, avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7155992",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, roc_auc = test(y_pred_df['y_actual'], y_pred_df['y_pred'])\n",
    "acc, f1, roc_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
