{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a5e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5422ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715763",
   "metadata": {},
   "source": [
    "#### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c7d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>87</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  width  height label\n",
       "0      25      2      77     A\n",
       "1      26     12      24     B\n",
       "2       5      4      16     B\n",
       "3      10      2      56     B\n",
       "4       5      6      87     B"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/balanced_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902d29ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  width  height  label\n",
       "0      25      2      77      0\n",
       "1      26     12      24      1\n",
       "2       5      4      16      1\n",
       "3      10      2      56      1\n",
       "4       5      6      87      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'A':0, 'B':1, 'C':2}\n",
    "df['label'] = df['label'].replace(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7270dd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1783\n",
       "1    1783\n",
       "0    1783\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471be578",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a34aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44e7e0",
   "metadata": {},
   "source": [
    "#### The Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7095e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.x = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.y = -1\n",
    "        self.num_classes = 3\n",
    "        self.sample_num = len(X)\n",
    "        self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.total_reward = 0\n",
    "        self.trajectory = []\n",
    "        self.episode_length = 0\n",
    "        self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "        \n",
    "    def reset(self, i): #I am going to go through the data sequentially\n",
    "        #print(f'Current epsiode completed. Resetting to index {i}')\n",
    "        if i < self.sample_num:\n",
    "            self.trajectory = []\n",
    "            self.total_reward = 0\n",
    "            self.episode_length = 0\n",
    "            self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "            self.x, self.y = self.X[i], self.Y[i]\n",
    "            self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "            #return self.state, self.available_actions\n",
    "            return self.state\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def get_next_state(self, action):\n",
    "        self.available_actions[0, action] =1\n",
    "        if action < 3: #the classes\n",
    "            next_state = None\n",
    "        elif (action >=3) & (action <=5):\n",
    "            feature_idx = action - 3\n",
    "            self.x = self.x.reshape(-1, 3)\n",
    "            x_value = self.x[0, feature_idx]\n",
    "            next_state = copy.deepcopy(self.state)\n",
    "            next_state[0, feature_idx] = x_value\n",
    "        return next_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        ep_length = 1\n",
    "        reward = 0\n",
    "        next_state = self.get_next_state(action)\n",
    "        if action < 3:\n",
    "            if action == self.y:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "            y_actual = self.y \n",
    "            y_pred = action\n",
    "            done = True\n",
    "        else:\n",
    "            reward += 0\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "            done=False\n",
    "            \n",
    "        self.total_reward+=reward\n",
    "        self.episode_length+= ep_length\n",
    "        total_reward_metric = self.total_reward \n",
    "        total_length_metric = self.episode_length\n",
    "        \n",
    "        info = {'episode_length':total_length_metric, 'total_reward': total_reward_metric, 'y_actual':y_actual, \n",
    "                   'y_pred': y_pred}\n",
    "        #print(f'The metrics: {metrics}')\n",
    "        return next_state, self.available_actions, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b567e",
   "metadata": {},
   "source": [
    "#### The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4985362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KugezesaAgent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=''):\n",
    "        self.state_size = state_size #the features are also 3\n",
    "        self.action_size=6 #length, width, height and the classes 0, 1, 2\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = load_model(model_name) if is_eval else self._model()\n",
    "    \n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=64, input_dim=self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(units=32, activation='relu'))\n",
    "        model.add(Dense(units=8, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss='mse', optimizer = Adam(learning_rate=0.001))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            print('Acting randomly')\n",
    "            return random.randrange(self.action_size)\n",
    "        options = self.model.predict(state)\n",
    "        print('Acting using q values')\n",
    "        return np.argmax(options[0])\n",
    "    \n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(1-batch_size+1, 1):\n",
    "            mini_batch.append(self.memory[i])\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward +self.gamma*np.amax(self.model.predict(next_state[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69823847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingEnv(Env):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__(X, y)\n",
    "        self.x, self.y = self.X[0], self.Y[0]\n",
    "        self.idx = 0\n",
    "        self.episode_length =len(self.trajectory)\n",
    "        \n",
    "    def step(self, action, name): \n",
    "        self.trajectory.append(name)\n",
    "        reward = 0\n",
    "        if action < 3:\n",
    "            if action == self.y:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "            done = True\n",
    "            y_actual = self.y \n",
    "            y_pred = action\n",
    "            self.idx+=1\n",
    "        \n",
    "        else:\n",
    "            reward += 0\n",
    "            done = False\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan \n",
    "        \n",
    "        next_state = self.get_next_state(action)\n",
    "        episode_number = self.idx\n",
    "        pathway = self.trajectory\n",
    "        self.total_reward += reward\n",
    "        total_reward_metric = self.total_reward \n",
    "        info = {'episode_length':len(pathway), 'total_reward': total_reward_metric, 'y_actual':y_actual, 'y_pred': y_pred, \n",
    "                'pathway': pathway, 'done':done} \n",
    "        return next_state, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36b5fa",
   "metadata": {},
   "source": [
    "#### The Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeca2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'info'))\n",
    "class Memory():\n",
    "    def __init__(self, size):\n",
    "        self.total_size = size\n",
    "        self.memory = deque([], maxlen = size)\n",
    "        self.keys = ['state', 'action', 'next_state', 'reward', 'done', 'info']\n",
    "        \n",
    "    def insert(self, *args):\n",
    "        transition = Transition(*args)\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        transition_samples = random.sample(self.memory, batch_size)\n",
    "        return transition_samples\n",
    "    \n",
    "    def get_last_n_samples(self, size): #compare output from sample and get_last_n samples\n",
    "        n_samples = []\n",
    "        for i in range(-1*size, 1):\n",
    "            n_samples.append(self.memory[i])\n",
    "        return n_samples\n",
    "    \n",
    "    def current_transitions(self):\n",
    "        '''Returns the number of current transitions in memory'''\n",
    "        return len(self.memory)\n",
    "        \n",
    "    def get_latest_transitions(self, transition_number): #I need to delete this i think but later\n",
    "        '''Get the last inserted transitions'''\n",
    "        transition_samples = []\n",
    "        current = self.current_transitions()\n",
    "        for i in range(current):\n",
    "            if i >= (current- transition_number):\n",
    "                transition_samples.append(self.memory[i])\n",
    "        return transition_samples\n",
    "\n",
    "    def reset(self):\n",
    "        ''' Resets the memory'''\n",
    "        self.memory = deque([], maxlen=self.total_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46387d",
   "metadata": {},
   "source": [
    "#### The NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12683902",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.policy_fn = torch.nn.Sequential( \n",
    "            torch.nn.Linear(input_size, hidden_size, bias=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size,hidden_size),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "        )\n",
    "        self.input_size = input_size\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = batch.view(-1, self.input_size) \n",
    "        x.to(device)\n",
    "        q_values = self.policy_fn(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2(torch.nn.Module):\n",
    "    def __init__(self, feature_num, action_num):\n",
    "        super(NN2, self).__init__()\n",
    "        \n",
    "        self.feature_num = feature_num\n",
    "        \n",
    "        self.layer_1 = torch.nn.Linear(feature_num, 512)\n",
    "        self.layer_2 = torch.nn.Linear(512, 128)\n",
    "        self.layer_3 = torch.nn.Linear(128, 64)\n",
    "        self.layer_out = torch.nn.Linear(64, action_num) \n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        #self.batchnorm1 = torch.nn.BatchNorm1d(512)\n",
    "        #self.batchnorm2 = torch.nn.BatchNorm1d(128)\n",
    "        #self.batchnorm3 = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.feature_num)\n",
    "        x = self.layer_1(x)\n",
    "        #x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer_2(x)\n",
    "        #x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        #x = self.batchnorm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e0fa3",
   "metadata": {},
   "source": [
    "#### The DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7293caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "class DQN():\n",
    "    def __init__(self, memory, input_size, hidden_size, output_size):\n",
    "        self.memory = memory\n",
    "        self.gamma=0.95\n",
    "        #self.policy_network = NN(input_size, hidden_size, output_size) #model\n",
    "        #self.target_network = NN(input_size, hidden_size, output_size) #_model\n",
    "        self.policy_network = NN2(3, 6)\n",
    "        self.target_network = NN2(3, 6)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        self.optimizer = optim.RMSprop(self.policy_network.parameters(), lr=0.00001) \n",
    "        \n",
    "        \n",
    "    def save_networks(self, filename): # I might need to delete this\n",
    "        torch.save(self.policy_network.state_dict(), f'{filename}_policy')\n",
    "        torch.save(self.target_network.state_dict(), f'{filename}_target')\n",
    "\n",
    "    def load_networks(self, filename):# I might need to delete this\n",
    "        if not os.path.exists(f'{filename}_policy'):\n",
    "            print('Folder from which to reload networks does not exist')\n",
    "        self.policy_network.load_state_dict(torch.load(f'{filename}_policy'))\n",
    "        self.target_network.load_state_dict(torch.load(f'{filename}_target'))\n",
    "        \n",
    "    def predict_q(self, state, target=False): #Target = true or false.. predict_np in classification\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        if target:\n",
    "            q = self.target_network(state)\n",
    "            q_values = q.detach().cpu().numpy() \n",
    "        else:\n",
    "            q = self.policy_network(state)\n",
    "            q_values = q.detach().cpu().numpy() \n",
    "        return q_values\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        \n",
    "    \n",
    "    #to transfer\n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        transitions_batch = self.memory.get_latest_transitions(batch_size)\n",
    "        batch = Transition(*zip(*transitions_batch))\n",
    "        non_final_mask = np.arrray(list(batch.done), dtype='bool')\n",
    "        non_final_next_states = np.array([batch.next_state[i] for i in range(batch_size) if batch.done[i] is False])\n",
    "        batch_states = np.array(batch.state)  \n",
    "        batch_actions = batch.action\n",
    "        batch_rewards = batch.reward\n",
    "        #state_action_values = self.policy_network(batch_states).gather(1, batch_actions.type(torch.int64).view(1, -1)) \n",
    "        state_action_values = self.policy_network.pedict(batch_states)\n",
    "        \n",
    "        next_state_values = np.zeros(batch_size, device=device)\n",
    "        next_state_values[~non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "        \n",
    "        expected_state_action_values = (self.gamma*next_state_values)+batch_rewards #the bellman equation\n",
    "        \n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        batch_states = np.array([transition[0] for transition in minibatch])#batch_states\n",
    "        state_action_values = self.model.predict(batch_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        transition_arr = [transition[3] for transition in minibatch]\n",
    "        print(transition_arr)\n",
    "        new_current_sta = np.array([transition[3] for transition in minibatch]).astype('float32')\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "    ###### END HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "        \n",
    "    def train_dqn(self, batch_size):\n",
    "        #print('Train dqn function')\n",
    "        #transitions_batch = self.memory.sample(batch_size) \n",
    "        transitions_batch = self.memory.get_latest_transitions(batch_size)\n",
    "        batch = Transition(*zip(*transitions_batch))\n",
    "        #print(f'Zipped transitions batch: {batch}')\n",
    "        non_final_mask = torch.tensor(list(batch.done), dtype=torch.bool).to(device=device) #e.g. [0,1,0] for 3 transitions\n",
    "        #print(f'Non final mask: {non_final_mask}')\n",
    "        non_final_next_states = torch.Tensor(np.array([batch.next_state[i] for i in range(batch_size) if batch.done[i] is False])).to(device=device)\n",
    "        #print(f'non final next states: {non_final_next_states}')\n",
    "        batch_states = torch.Tensor(np.array(batch.state)).to(device=device)\n",
    "        #print(f'batch states: {batch_states}')\n",
    "        batch_actions = torch.Tensor(batch.action).to(device=device)\n",
    "        #print(f'batch actions: {batch_actions}')\n",
    "        batch_rewards = torch.Tensor(batch.reward).to(device=device)\n",
    "        #print(f'batch rewards: {batch_rewards}')\n",
    "               \n",
    "        \n",
    "\n",
    "        state_action_values = self.policy_network(batch_states).gather(1, batch_actions.type(torch.int64).view(1, -1))  \n",
    "        #print(f'state_action_values shape: {state_action_values.shape}')\n",
    "        #print(f'state_action_values: {state_action_values}')\n",
    "        \n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "        next_state_values[~non_final_mask] = self.target_network(non_final_next_states).max(1)[0].detach()\n",
    "        #print(f'next_state_values shape: {next_state_values.shape}')\n",
    "        #print(f'next state values: {next_state_values}')\n",
    "        \n",
    "        expected_state_action_values = (self.gamma*next_state_values)+batch_rewards #the bellman equation\n",
    "        #print(f'expected state action values shape: {expected_state_action_values.shape}')\n",
    "        #print(f'expected state action values: {expected_state_action_values}')\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        #mse_loss = mse(state_action_values, expected_state_action_values.unsqueeze(1)) - original comment\n",
    "        loss = criterion(state_action_values, expected_state_action_values.view(1, -1))\n",
    "        #self.entropy_loss = torch.nn.CrossEntropyLoss() #original comment\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step() \n",
    "        #print(f'MSE loss: {loss.item()}')\n",
    "        #return {'loss': {loss.item()}} #will probaly change this return statement\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Keras():\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dropout(0.5, input_shape=(1, 3)))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(6, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "     def __init__(self, memory, input_size, hidden_size, output_size):\n",
    "        self.memory = memory\n",
    "        self.gamma=0.95\n",
    "        #self.policy_network = NN(input_size, hidden_size, output_size) #model\n",
    "        #self.target_network = NN(input_size, hidden_size, output_size) #_model\n",
    "        self.policy_network = create_model()\n",
    "        self.target_network = create_model()\n",
    "        self.target_model.set_weights(self.policy_network.get_weights())\n",
    "        #self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        #self.target_network.eval()\n",
    "        #self.optimizer = optim.RMSprop(self.policy_network.parameters(), lr=0.00001) \n",
    "        \n",
    "        \n",
    "    def save_networks(self, filename): # I might need to delete this\n",
    "        torch.save(self.policy_network.state_dict(), f'{filename}_policy')\n",
    "        torch.save(self.target_network.state_dict(), f'{filename}_target')\n",
    "\n",
    "    def load_networks(self, filename):# I might need to delete this\n",
    "        if not os.path.exists(f'{filename}_policy'):\n",
    "            print('Folder from which to reload networks does not exist')\n",
    "        self.policy_network.load_state_dict(torch.load(f'{filename}_policy'))\n",
    "        self.target_network.load_state_dict(torch.load(f'{filename}_target'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46f3ec",
   "metadata": {},
   "source": [
    "#### The Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9bd8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, memory, dqn):\n",
    "        self.env = env\n",
    "        self.dqn = dqn\n",
    "        self.memory = self.dqn.memory\n",
    "        \n",
    "        self.action_space = ['A', 'B', 'C', 'length', 'width', 'height']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        #self.epsilon = 0.1 #slot_set of dialogue system will come later\n",
    "        #self.epsilon = 1.0\n",
    "        #self.epsilon_start = 1.0   #for epsilon-greedy - chosen arbitrarily -- this is for epsilon start\n",
    "        #self.epsilon_end = 0.1\n",
    "        #self.epsilon_decay = 200\n",
    "        self.available_actions = self.env.available_actions\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "    def get_action(self, state, available_actions): #what action is taken next by agent in this state\n",
    "        sample = random.random() \n",
    "        #print(f'Epsilon: {self.epsilon}')\n",
    "        if sample > self.epsilon:\n",
    "            #print('Using q values')\n",
    "            with torch.no_grad():\n",
    "                q_values = self.dqn.predict_q(state) #get q values for all actions shape(1, 31)\n",
    "                available_q_values = q_values[available_actions==0] #q-values for actions not yet selected e.g. shape(20,)\n",
    "                action_q = np.max(available_q_values)\n",
    "                #action_q = np.max(q_values)\n",
    "                action_index = np.where(q_values[0]==action_q)[0][0] #index of action with max q-value\n",
    "        else:\n",
    "            #print('Choosing randomly')\n",
    "            available_indices = np.where(available_actions==0)[1]\n",
    "            action_index = random.choice(available_indices)  \n",
    "            #action_index = random.randrange(self.n_actions)\n",
    "        action_name = self.action_space[action_index]\n",
    "        return action_index, action_name\n",
    "    \n",
    "    def update_epsilon(self, epoch_number):\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * epoch_number / self.epsilon_decay)\n",
    "        \n",
    "    def update_epsilon2(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def step(self):\n",
    "        #print('Step by agent')\n",
    "        current_state = self.env.state\n",
    "        #print(f'The available actions used to get action: {self.env.available_actions}')\n",
    "        action_index = self.get_action(current_state, self.env.available_actions)[0]\n",
    "        next_state, self.available_actions, reward, done, info = self.env.step(action_index)\n",
    "        #print('inserting into memory')\n",
    "        #print(f'Current state: {current_state}')\n",
    "        #print(f'action_index: {action_index}')\n",
    "        #print(f'Next state: {next_state}')\n",
    "        #print(f'Reward: {reward}')\n",
    "        #print(f'Done: {done}')\n",
    "        #print(f'Info: {info}')\n",
    "        self.memory.insert(current_state, action_index, next_state, reward, done, info)\n",
    "        if next_state is None:\n",
    "            i = random.randint(0, self.env.sample_num-1)\n",
    "            self.env.reset(i)\n",
    "        else:\n",
    "            self.env.state = next_state\n",
    "            \n",
    "    def test(self):\n",
    "        test_df = pd.DataFrame()\n",
    "        while self.env.idx < self.env.sample_num:\n",
    "            #print(f'INDEX: {self.env.idx}')\n",
    "            action_index, action_name, current_state, next_state, info = self.step()\n",
    "            #print(f'Current state: {current_state}')\n",
    "            #print(f'action index: {action_index}, action name: {action_name}')\n",
    "            #print(f'Next state: {next_state}')\n",
    "            #print(f'Info: {info}')\n",
    "            if info['done']==True:\n",
    "                test_df = test_df.append(info, ignore_index=True)\n",
    "        return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f20e7",
   "metadata": {},
   "source": [
    "#### The Testing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38150d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingAgent(Agent):\n",
    "    def __init__(self, env, memory, dqn):\n",
    "        super().__init__(env, memory, dqn)\n",
    "    \n",
    "    def get_action(self, state, available_actions): \n",
    "        q_values = self.dqn.predict_q(state) \n",
    "        available_q_values = q_values[available_actions==0]\n",
    "        action_q = np.max(available_q_values) \n",
    "        action_index = np.where(q_values[0]==action_q)[0][0]  \n",
    "        #action_index = np.argmax(q_values[0]) \n",
    "        action_name = self.action_space[action_index]\n",
    "        return action_index, action_name\n",
    "\n",
    "    def load_networks(self, filename):\n",
    "        self.dqn.load_networks(filename)\n",
    "\n",
    "    def step(self): #state   ------ not finished where is done?\n",
    "        current_state = self.env.state\n",
    "        action_index, action_name = self.get_action(current_state, self.env.available_actions)\n",
    "        next_state, info = self.env.step(action_index, action_name)\n",
    "        if next_state is None:\n",
    "            self.env.reset(self.env.idx)\n",
    "        else:\n",
    "            self.env.state = next_state\n",
    "        return action_index, action_name, current_state, next_state, info\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31c069",
   "metadata": {},
   "source": [
    "#### The Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4029b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "daa09a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "\n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "    #print(f'Roc auc dict: {roc_auc_dict}')\n",
    "    avg = sum(roc_auc_dict.values()) / len(roc_auc_dict)\n",
    "    \n",
    "    #return roc_auc_dict\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4664886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ytest, ypred):\n",
    "    acc = accuracy_score(ytest, ypred)\n",
    "    f1 = f1_score(ytest, ypred, average ='macro', labels=np.unique(ytest))\n",
    "    try:\n",
    "        roc_auc = multiclass(ytest, ypred)\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    return acc, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a388385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance(X_test, y_test, dqn, isTest=False):\n",
    "    testing_env = TestingEnv(X_test, y_test)\n",
    "    testing_agent = TestingAgent(testing_env, None, dqn)\n",
    "    if isTest:\n",
    "        testing_agent.dqn.load_networks('models/model')\n",
    "    test_df = testing_agent.test()\n",
    "    acc, f1, roc_auc = test(test_df.y_actual, test_df.y_pred)\n",
    "    return acc, f1, roc_auc, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85361401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, y_train, X_val, y_val, X_test, y_test, epochs, epoch_steps, batch_size):\n",
    "    memory = Memory(1000)\n",
    "    env = Env(X_train, y_train)\n",
    "    dqn = DQN(memory, 3, 64, 6)\n",
    "    agent = Agent(env, memory, dqn)\n",
    "    \n",
    "    best_val_perf= {'accuracy':0, 'f1':0, 'roc_auc_score':0}\n",
    "\n",
    "    print('INITIALIZING MEMORY .....') #maybe skip this and go straight to the \n",
    "    i = random.randint(0, len(X_train)-1)  #try starting wit zero too and moving in a sequential manner\n",
    "    env.reset(i)\n",
    "    while memory.current_transitions() < memory.total_size:\n",
    "    #while memory.current_transitions() < 5:\n",
    "        agent.step()\n",
    "        \n",
    "    print('TRAINING')\n",
    "    for epoch in range(epochs):\n",
    "        loss = dqn.train_dqn(batch_size)\n",
    "        #print(f'loss: {loss}')\n",
    "        if epoch%5 ==0:\n",
    "            dqn.update_target_network()\n",
    "        #if epoch%1 == 0:\n",
    "            #agent.update_epsilon(epoch)\n",
    "        agent.update_epsilon2()\n",
    "        for i in range(epoch_steps):\n",
    "            agent.step()\n",
    "        #if epoch%(epochs/100) ==0:            \n",
    "        validation_perf = get_performance(X_val, y_val, dqn)\n",
    "        if validation_perf[0] > best_val_perf['accuracy']:\n",
    "            dqn.save_networks('models/model')\n",
    "            best_val_perf['accuracy'] = validation_perf[0]\n",
    "            best_val_perf['f1'] = validation_perf[1]\n",
    "            best_val_perf['roc_auc_score'] = validation_perf[2]\n",
    "\n",
    "            print(f'********Validation Performance at epoch {epoch}********')\n",
    "            print(f'Accuracy: {validation_perf[0]}, F1 score: {validation_perf[1]}, ROC-AUC Score: {validation_perf[2]}')\n",
    "            print(f'Unique predicted classes: {validation_perf[3].y_pred.unique()}')\n",
    "            \n",
    "            test_perf = get_performance(X_test, y_test, dqn, isTest=True)\n",
    "            \n",
    "            #dqn.memory.reset\n",
    "\n",
    "    \n",
    "    print('TESTING')\n",
    "    test_perf = get_performance(X_test, y_test, dqn, isTest=True)\n",
    "    print('**********Test Performance**********')\n",
    "    print(f'Accuracy: {test_perf[0]}, F1 score: {test_perf[1]}, ROC-AUC Score: {test_perf[2]}')\n",
    "    print(f'Unique predicted classes: {test_perf[3].y_pred.unique()}')\n",
    "    \n",
    "    return test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554f2be",
   "metadata": {},
   "source": [
    "#### delete from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2600466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba19a6f",
   "metadata": {},
   "source": [
    "#### All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55ef558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING MEMORY .....\n",
      "TRAINING\n",
      "********Validation Performance at epoch 0********\n",
      "Accuracy: 0.32710280373831774, F1 score: 0.18071211307926283, ROC-AUC Score: 0.4947491105385842\n",
      "Unique predicted classes: [0. 1.]\n",
      "********Validation Performance at epoch 1********\n",
      "Accuracy: 0.35046728971962615, F1 score: 0.22154607297464443, ROC-AUC Score: 0.5122520754099702\n",
      "Unique predicted classes: [0. 1.]\n",
      "********Validation Performance at epoch 25********\n",
      "Accuracy: 0.36682242990654207, F1 score: 0.26314878534650893, ROC-AUC Score: 0.524512358574627\n",
      "Unique predicted classes: [0. 1. 2.]\n",
      "********Validation Performance at epoch 101********\n",
      "Accuracy: 0.3691588785046729, F1 score: 0.26432634242902603, ROC-AUC Score: 0.5262544473070788\n",
      "Unique predicted classes: [0. 1.]\n",
      "********Validation Performance at epoch 104********\n",
      "Accuracy: 0.3855140186915888, F1 score: 0.27459311329195474, ROC-AUC Score: 0.5385188199495093\n",
      "Unique predicted classes: [0. 1. 2.]\n",
      "********Validation Performance at epoch 115********\n",
      "Accuracy: 0.397196261682243, F1 score: 0.30964335696589745, ROC-AUC Score: 0.5472785101398889\n",
      "Unique predicted classes: [0. 1. 2.]\n",
      "********Validation Performance at epoch 157********\n",
      "Accuracy: 0.4182242990654206, F1 score: 0.3342525232057357, ROC-AUC Score: 0.5630352680019101\n",
      "Unique predicted classes: [1. 0. 2.]\n",
      "********Validation Performance at epoch 336********\n",
      "Accuracy: 0.42289719626168226, F1 score: 0.34227141450059256, ROC-AUC Score: 0.5666302069563746\n",
      "Unique predicted classes: [0. 1. 2.]\n",
      "********Validation Performance at epoch 840********\n",
      "Accuracy: 0.42990654205607476, F1 score: 0.39712012271114944, ROC-AUC Score: 0.571995889786846\n",
      "Unique predicted classes: [0. 1. 2.]\n",
      "TESTING\n",
      "**********Test Performance**********\n",
      "Accuracy: 0.35700934579439253, F1 score: 0.32698584872497916, ROC-AUC Score: 0.5179028795122695\n",
      "Unique predicted classes: [1. 0. 2.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>done</th>\n",
       "      <th>episode_length</th>\n",
       "      <th>pathway</th>\n",
       "      <th>total_reward</th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[length, height, width, B]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[length, height, width, A]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[length, width, height, A]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[height, length, width, B]</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[height, length, width, A]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   done  episode_length                     pathway  total_reward  y_actual  \\\n",
       "0   1.0             4.0  [length, height, width, B]          -1.0       0.0   \n",
       "1   1.0             4.0  [length, height, width, A]          -1.0       2.0   \n",
       "2   1.0             4.0  [length, width, height, A]          -1.0       1.0   \n",
       "3   1.0             4.0  [height, length, width, B]          -1.0       2.0   \n",
       "4   1.0             4.0  [height, length, width, A]           1.0       0.0   \n",
       "\n",
       "   y_pred  \n",
       "0     1.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     1.0  \n",
       "4     0.0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_perf = main(X_train, y_train, X_val, y_val, X_test, y_test, 1000, 5, 32)\n",
    "test_df = test_perf[3]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57b2c33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    357\n",
       "2.0    357\n",
       "0.0    356\n",
       "Name: y_actual, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.y_actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8dfcf52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    541\n",
       "1.0    405\n",
       "2.0    124\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.y_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40035d3e",
   "metadata": {},
   "source": [
    "#### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87db0146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9710280373831774"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(test_df.episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc56ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5c55c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
