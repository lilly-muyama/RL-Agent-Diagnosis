{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b34c553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ff70900",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25835f",
   "metadata": {},
   "source": [
    "#### The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53569f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/balanced_q_dataset.csv')\n",
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=42)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d24d52",
   "metadata": {},
   "source": [
    "#### The environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e00ae421",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.x = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.y = -1\n",
    "        self.num_classes = 3\n",
    "        self.sample_num = len(X)\n",
    "        self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.total_reward = 0\n",
    "        self.trajectory = []\n",
    "        self.episode_length = 0\n",
    "        self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "        \n",
    "    def reset(self): #I am going to go through the data sequentially\n",
    "        #print(f'Current epsiode completed. Resetting to index {i}')\n",
    "        #if i < self.sample_num:\n",
    "        i = random.randint(0, self.env.sample_num-1)\n",
    "        self.trajectory = []\n",
    "        self.total_reward = 0\n",
    "        self.episode_length = 0\n",
    "        self.state = np.zeros((1, 3), dtype=np.float32)\n",
    "        self.x, self.y = self.X[i], self.Y[i]\n",
    "        self.available_actions = np.zeros((1, 6), dtype=np.float32)\n",
    "        return self.state, self.available_actions\n",
    "       # else:\n",
    "       #     pass\n",
    "        \n",
    "    def get_next_state(self, action):\n",
    "        self.available_actions[0, action] =1\n",
    "        if action < 3: #the classes\n",
    "            next_state = None\n",
    "        elif (action >=3) & (action <=5):\n",
    "            feature_idx = action - 3\n",
    "            self.x = self.x.reshape(-1, 3)\n",
    "            x_value = self.x[0, feature_idx]\n",
    "            next_state = copy.deepcopy(self.state)\n",
    "            next_state[0, feature_idx] = x_value\n",
    "        return next_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        ep_length = 1\n",
    "        reward = 0\n",
    "        next_state = self.get_next_state(action)\n",
    "        if action < 3:\n",
    "            if action == self.y:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "            y_actual = self.y \n",
    "            y_pred = action\n",
    "            done = True\n",
    "        else:\n",
    "            reward += 0\n",
    "            y_actual = np.nan\n",
    "            y_pred = np.nan\n",
    "            done=False\n",
    "            \n",
    "        self.total_reward+=reward\n",
    "        self.episode_length+= ep_length\n",
    "        total_reward_metric = self.total_reward \n",
    "        total_length_metric = self.episode_length\n",
    "        \n",
    "        info = {'episode_length':total_length_metric, 'total_reward': total_reward_metric, 'y_actual':y_actual, \n",
    "                   'y_pred': y_pred}\n",
    "        #print(f'The metrics: {metrics}')\n",
    "        return next_state, self.available_actions, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174e4b7",
   "metadata": {},
   "source": [
    "#### The neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6980c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = 6\n",
    "nb_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecb57067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                64        \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 6)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166\n",
      "Trainable params: 166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, nb_features)))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abe49622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\phd2_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "env = Env(X_train, y_train)\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af340bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Env' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23332\\2207363274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\phd2_env\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[1;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                     \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23332\\300303259.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#print(f'Current epsiode completed. Resetting to index {i}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#if i < self.sample_num:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_num\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrajectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Env' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8398e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446c36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
