{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198f68be",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "baa00aea",
   "metadata": {},
   "source": [
    "In this code, we load data from a CSV file and use it to define the state and action sizes for our environment. We then define a custom environment class that uses the data to provide observations and rewards. We create the environment, normalize its observations, and define and train a DQN agent using the learn method. Finally, we save the trained DQN agent to a file. Note that this is just a sample code, and you may need to modify it to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('diagnosis_data.csv')\n",
    "state_size = data.shape[1] - 1\n",
    "action_size = 1\n",
    "\n",
    "# Define the custom environment\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(state_size,))\n",
    "        self.action_space = gym.spaces.Discrete(action_size)\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        if self.current_step == self.max_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        reward = self.data.iloc[self.current_step,state_size+action_size]\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Create the environment\n",
    "env = DiagnosisEnv(data)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "# Define and train the DQN agent\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained DQN agent\n",
    "model.save('dqn_diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962d742",
   "metadata": {},
   "source": [
    "#### PPO"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f6a44ae",
   "metadata": {},
   "source": [
    "Question: write a sample python code using a policy gradient method from a python reinforcement learning library and data in an csv file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3248cc8",
   "metadata": {},
   "source": [
    "Answer: In this example, we load the data from a CSV file, define a Gym environment that is suitable for the diagnosis problem, and use the PPO2 algorithm from the Stable Baselines library to train a policy. We then evaluate the policy by running it on the environment until a termination condition is met. Note that this is a simplified example and that in practice, you would need to carefully design the environment and objective function for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Define the Gym environment based on the data\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(data.shape[1],))\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.current_state = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self._get_observation()\n",
    "        \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:\n",
    "            # The patient is diagnosed as positive for the condition\n",
    "            done = True\n",
    "            if data.iloc[self.current_state]['label'] == 1:\n",
    "                # True positive\n",
    "                reward = 10\n",
    "            else:\n",
    "                # False positive\n",
    "                reward = -10\n",
    "        else:\n",
    "            # The patient is diagnosed as negative for the condition\n",
    "            self.current_state += 1\n",
    "            if self.current_state >= len(data):\n",
    "                done = True\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        return data.iloc[self.current_state].drop('label').values\n",
    "\n",
    "# Create the Gym environment\n",
    "env = DummyVecEnv([lambda: DiagnosisEnv()])\n",
    "\n",
    "# Train the policy using PPO\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate the policy\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, done, _ = env.step(action)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfb8af",
   "metadata": {},
   "source": [
    "#### Actor Critic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "198d07c8",
   "metadata": {},
   "source": [
    "In this example, we load the data from a CSV file, define a Gym environment that is suitable for the diagnosis problem, and use the A2C algorithm from the Stable Baselines library to train an actor-critic policy. We then evaluate the policy by running it on the environment until a termination condition is met. Note that this is a simplified example and that in practice, you would need to carefully design the environment and objective function for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Define the Gym environment based on the data\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(data.shape[1],))\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.current_state = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        return self._get_observation()\n",
    "        \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 1:\n",
    "            # The patient is diagnosed as positive for the condition\n",
    "            done = True\n",
    "            if data.iloc[self.current_state]['label'] == 1:\n",
    "                # True positive\n",
    "                reward = 10\n",
    "            else:\n",
    "                # False positive\n",
    "                reward = -10\n",
    "        else:\n",
    "            # The patient is diagnosed as negative for the condition\n",
    "            self.current_state += 1\n",
    "            if self.current_state >= len(data):\n",
    "                done = True\n",
    "        \n",
    "        return self._get_observation(), reward, done, {}\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        return data.iloc[self.current_state].drop('label').values\n",
    "\n",
    "# Create the Gym environment\n",
    "env = DummyVecEnv([lambda: DiagnosisEnv()])\n",
    "\n",
    "# Train the policy using A2C\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate the policy\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, done, _ = env.step(action)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff0b18",
   "metadata": {},
   "source": [
    "#### DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95419b",
   "metadata": {},
   "source": [
    "n this code, we load data from a CSV file and use it to define the state and action sizes for our environment. We then build actor and critic models using Keras layers, and define a DDPG agent using those models and other hyperparameters. We compile the agent and then train it on the data using the fit method. Finally, we save the weights of the trained agent to a file. Note that this is just a sample code, and you may need to modify it to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a265d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stable-baselines3 pandas gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17152ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the CSV file has columns for the patient's symptoms and the corresponding diagnosis, we can use this\n",
    "#data to create a custom Gym environment that implements the diagnosis problem as a Markov Decision Process (MDP). \n",
    "#Here's an example implementation of the DiagnosisEnv class\n",
    "import pandas as pd\n",
    "data = pd.read_csv('diagnosis_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DiagnosisEnv class has an observation space that represents the patient's symptoms as a vector of binary \n",
    "#values, and an action space that represents the possible diagnoses as discrete values. Each step of the environment\n",
    "#corresponds to a new patient, and the reward is 1 if the chosen diagnosis matches the true diagnosis and 0 \n",
    "#otherwise. Now we can use this environment to train a DDPG agent using the Stable Baselines3 library:\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.action_space = spaces.Discrete(len(data['diagnosis'].unique()))\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(data.columns)-1,))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state = self.data.iloc[self.current_step, :-1].values\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        reward = 0\n",
    "        done = self.current_step >= len(self.data)\n",
    "        next_state = self.data.iloc[self.current_step, :-1].values\n",
    "        diagnosis = self.data.iloc[self.current_step - 1, -1]\n",
    "        if action == diagnosis:\n",
    "            reward = 1\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code creates a DDPG agent with a multi-layer perceptron (MLP) policy, trains it for 10,000 timesteps, and \n",
    "#saves checkpoints of the model every 1,000 timesteps. Finally, we can use the trained agent to make predictions \n",
    "#on new patients:\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "env = DiagnosisEnv(data)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "action_noise = NormalActionNoise(mean=0, sigma=0.1 * n_actions)\n",
    "model = DDPG('MlpPolicy', env, action_noise=action_noise, verbose=1)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')\n",
    "\n",
    "model.learn(total_timesteps=10000, callback=checkpoint_callback)\n",
    "\n",
    "#This code creates a new patient with a vector of binary symptom values, passes it to the trained agent's predict() \n",
    "#method to get a diagnosis action, and then maps the action back to a diagnosis label using the data from the CSV \n",
    "#file.\n",
    "import numpy as np\n",
    "\n",
    "patient = np.array([1, 0, 0, 1, 1, 0, 1, 0, 1])\n",
    "action, _ = model.predict(patient)\n",
    "diagnosis = data['diagnosis'].unique()[action]\n",
    "print(diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb60280",
   "metadata": {},
   "source": [
    "#### A3C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81a64",
   "metadata": {},
   "source": [
    "In this code, we load data from a CSV file and use it to define the state and action sizes for our environment. We then build actor and critic models using Keras layers, and define an A3C agent using those models and other hyperparameters. We compile the agent and then train it on the data using the fit method. Finally, we save the weights of the trained agent to a file. Note that this is just a sample code, and you may need to modify it to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20457d41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136495/990250074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Define the diagnosis environment\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self, csv_file):\n",
    "        super(DiagnosisEnv, self).__init__()\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(self.df.columns)-1,))\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.episode_length = len(self.df)\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        return self.df.iloc[self.current_step, :-1].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        state = self.df.iloc[self.current_step, :-1].values\n",
    "        reward = self.df.iloc[self.current_step, -1] if action == 1 else 0\n",
    "        self.current_step += 1\n",
    "        if self.current_step == self.episode_length:\n",
    "            self.done = True\n",
    "        return state, reward, self.done, {}\n",
    "        \n",
    "\n",
    "# Define the A3C agent\n",
    "class A3C(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(A3C, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.policy_logits = tf.keras.layers.Dense(action_size)\n",
    "        self.values = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        logits = self.policy_logits(x)\n",
    "        values = self.values(x)\n",
    "        return logits, values\n",
    "    \n",
    "\n",
    "# Train the A3C agent\n",
    "env = DiagnosisEnv('diagnosis_data.csv')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = A3C(state_size, action_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "huber_loss = tf.keras.losses.Huber()\n",
    "entropy_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(state, action, reward, next_state, done):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, values = model(state)\n",
    "        next_logits, _ = model(next_state)\n",
    "        \n",
    "        td_target = reward + 0.99 * next_logits[0, tf.argmax(logits, axis=1)[0]] * (1 - done)\n",
    "        td_error = td_target - values[0]\n",
    "        \n",
    "        critic_loss = huber_loss(tf.expand_dims(td_error, 0), tf.zeros((1, 1)))\n",
    "        \n",
    "        action_one_hot = tf.one_hot(action, action_size)\n",
    "        entropy = tf.reduce_sum(tf.nn.softmax(logits) * tf.math.log(tf.nn.softmax(logits)))\n",
    "        actor_loss = entropy_loss(action_one_hot, logits) - 0.001 * entropy\n",
    "        \n",
    "        total_loss = critic_loss + actor_loss\n",
    "        \n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b842c",
   "metadata": {},
   "source": [
    "#### TD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7d83f",
   "metadata": {},
   "source": [
    "In this code, we load data from a CSV file and use it to define the state and action sizes for our environment. We then define a custom environment class that uses the data to provide observations and rewards. We create the environment, normalize its observations, and define and train a TD3 agent using the learn method. Finally, we save the trained TD3 agent to a file. Note that this is just a sample code, and you may need to modify it to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('diagnosis_data.csv')\n",
    "state_size = data.shape[1] - 1\n",
    "action_size = 1\n",
    "\n",
    "# Define the custom environment\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(state_size,))\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(action_size,))\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        if self.current_step == self.max_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        reward = self.data.iloc[self.current_step,state_size+action_size]\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Create the environment\n",
    "env = DiagnosisEnv(data)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "# Define and train the TD3 agent\n",
    "model = TD3('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained TD3 agent\n",
    "model.save('td3_diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576556e",
   "metadata": {},
   "source": [
    "#### TRPO Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e8ab249",
   "metadata": {},
   "source": [
    "In this code, we load data from a CSV file and use it to define the state and action sizes for our environment. We then define a custom environment class that uses the data to provide observations and rewards. We create the environment, normalize its observations, and define and train a TRPO agent using the learn method. Finally, we save the trained TRPO agent to a file. Note that this is just a sample code, and you may need to modify it to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c230f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from stable_baselines3 import TRPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv('diagnosis_data.csv')\n",
    "state_size = data.shape[1] - 1\n",
    "action_size = 1\n",
    "\n",
    "# Define the custom environment\n",
    "class DiagnosisEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(state_size,))\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(action_size,))\n",
    "        self.current_step = 0\n",
    "        self.max_steps = len(self.data)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        if self.current_step == self.max_steps:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = self.data.iloc[self.current_step,:state_size].to_numpy()\n",
    "        reward = self.data.iloc[self.current_step,state_size+action_size]\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# Create the environment\n",
    "env = DiagnosisEnv(data)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "# Define and train the TRPO agent\n",
    "model = TRPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the trained TRPO agent\n",
    "model.save('trpo_diagnosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2062f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
